[{"xtie/Clinicallongformer2roberta-PET-impression": []}, {"xtie/BARTScore-PET": ["## \ud83d\udcd1 Model Description", "## \ud83d\ude80 Usage", "## \ud83d\udcc1 Additional Resources"]}, {"TheBloke/medalpaca-13B-AWQ": ["## Description", "### About AWQ", "## Repositories available", "## Prompt template: Alpaca", "### Instruction:", "### Response:", "## Provided files and AWQ parameters", "## Serving this model from vLLM", "## How to use this AWQ model from Python code", "### Install the necessary packages", "### You can then try the following example code", "### Instruction:", "### Response:", "## Compatibility", "## Discord", "## Thanks, and how to contribute", "## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Model Usage", "## Limitations"]}, {"ahmed-ai/galen": ["### Galen's view about future of medicine and AI:"]}, {"sidushdid/ViT-base-patch16-BUSI-Mendeley-AdamW": []}, {"knowledgator/SMILES-DeBERTa-small": ["## Model Details", "### Model Description", "## Citation", "## Model Card Authors", "## Model Card Contact"]}, {"GeorgiaTech/bert-generative-pubmedqa": []}, {"Roselia-penguin/medical_llama3_8b": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"darragh/swinunetr-btcv-tiny": []}, {"darragh/swinunetr-btcv-small": []}, {"darragh/swinunetr-btcv-base": []}, {"xtie/BioBART-PET-impression": ["## \ud83d\udcd1 Model Description", "## \ud83d\udcd1 Abstract", "## \ud83d\ude80 Usage", "### \ud83d\udcca Performance Metrics", "### \ud83d\udca1 Highlights", "### \ud83d\udda5\ufe0f Hardware", "## \ud83d\udcc1 Additional Resources"]}, {"xtie/T5Score-PET": ["## \ud83d\udcd1 Model Description", "## \ud83d\udcc1 Additional Resources"]}, {"Mreeb/Medi-llama-2-7b-custom100": ["## Model Details", "### Model Description"]}, {"LoneStriker/medicine-LLM-13B-4.0bpw-h6-exl2": ["### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"AIgroup-CVM-utokyohospital/MedSwallow-70b": ["## Training procedure", "### Framework versions", "## License", "## Usage", "## Benchmark", "## How to cite"]}, {"winninghealth/WiNGPT2-7B-Chat-AWQ": ["## WiNGPT2", "## \u66f4\u65b0\u65e5\u5fd7", "## \u4ecb\u7ecd", "## \u7279\u70b9", "## \u5982\u4f55\u4f7f\u7528", "### \u63a8\u7406", "## \u8f93\u51fa\u7ed3\u679c\uff1a\u4f60\u597d\uff01\u4eca\u5929\u6211\u80fd\u4e3a\u4f60\u505a\u4e9b\u4ec0\u4e48\uff1f<|endoftext|>", "### \u63d0\u793a", "### \u4f01\u4e1a\u670d\u52a1", "## \u8bad\u7ec3\u6570\u636e", "## \u6a21\u578b\u5361", "## \u8bc4\u6d4b", "## \u5c40\u9650\u6027\u4e0e\u514d\u8d23\u58f0\u660e", "## \u8bb8\u53ef\u8bc1", "## \u53c2\u8003\u8d44\u6599", "## \u8054\u7cfb\u6211\u4eec"]}, {"SaborDay/Phi2_RCT1M-ft-heading": ["## Model Details", "### Model Sources [optional]", "## Uses", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Training Hyperparameters", "#### Training Run metrics", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Metrics", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Model Card Contact", "## References"]}, {"yuxindu/segvol": ["## Quicktart", "### Requirements", "### Test script", "### Training script"]}, {"SJChaudhuri/Efficient-MaxViT": []}, {"therealcyberlord/llama2-qlora-finetuned-medical": ["## Training procedure", "### Framework versions"]}, {"shuvom/falcon-med-FT-v1.114": ["## Training procedure", "### Framework versions", "### Citation"]}, {"xtie/PEGASUSScore-PET": ["## \ud83d\udcd1 Model Description", "## \ud83d\udcc1 Additional Resources"]}, {"cxllin/Llama2-7b-med-v1": ["## Model Details", "### Description", "#### Development Details", "### Model Source Links", "### Direct Applications", "### Downstream Applications", "### Out-of-Scope Utilizations", "## Bias, Risks, and Limitations", "### Recommendations for Use", "## Getting Started with the Model", "### Training Dataset", "#### Preprocessing Steps"]}, {"jonathanjordan21/donut-finetuned-drugs-composition-indonesian": ["## Model description", "## Usage & limitations", "### Output Example", "### How to use"]}, {"Tonic/stablemed": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Results", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Model Card Authors [optional]", "## Model Card Contact", "## Training procedure", "### Framework versions"]}, {"ngwlh/KBioXLM": ["## Model description", "## Usage", "### BibTeX entry and citation info"]}, {"LoneStriker/medicine-LLM-13B-3.0bpw-h6-exl2": ["### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"Naati101/bioscan_tb": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters"]}, {"knowledgator/SMILES2IUPAC-canonical-small": ["## Model Details", "### Model Description", "### Model Sources", "## Quickstart", "### SMILES to IUPAC", "#### ! Preferred IUPAC style", "#### To perform simple translation, follow the example:", "#### Processing in batches:", "#### Validation SMILES to IUPAC translations", "## Bias, Risks, and Limitations", "### Training Procedure", "## Evaluation", "## Citation", "## Model Card Authors", "## Model Card Contact"]}, {"LoneStriker/BioMistral-7B-DARE-8.0bpw-h8-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"Amod/mental-health-therapy-mistral-7b-ins-SFT": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"abdibrokhim/gemma-fine-tuned-brainmri-2402": ["### Model Sources", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"saqlainshah/gemma_2b_finetuned_medal": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"cmcmaster/il_7b": ["## Intended Use", "## Merge Details", "### Merge Method", "### Models Merged"]}, {"HiTZ/Medical-mT5-xl-multitask": []}, {"BMRetriever/BMRetriever-1B": ["## Usage", "## Citation"]}, {"Henrychur/MMed-Llama-3-8B-EnIns": ["## Introduction", "## News", "## Evaluation on Commonly-used English Benchmark", "## Contact", "## Citation"]}, {"Reverb/MedLLaMA-3": ["## \ud83d\udcbb Usage", "## \ud83c\udfc6 Evaluation"]}, {"Su-informatics-lab/bert_base_uncased_rxnorm_babbage": ["## Overview ", "## Training", "## Usage ", "## License"]}, {"TheBloke/Asclepius-13B-GGML": ["## Description", "### Important note regarding GGML files.", "### About GGML", "## Repositories available", "## Prompt template: Asclepius", "## Compatibility", "## Explanation of the new k-quant methods", "## Provided files", "## How to run in `llama.cpp`", "## How to run in `text-generation-webui`", "## Discord", "## Thanks, and how to contribute.", "## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Citation [optional]"]}, {"siddharthbulia/therapy-bot": ["## Nintee Therapy Bot"]}, {"cardiologs-hackathon/invertnet-hackathon": []}, {"DaizeDong/GraphsGPT-8W": []}, {"Naati101/tb": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters"]}, {"fhai50032/Johan-7B-v0.1": ["## Model Details", "## Training Details"]}, {"MilosKosRad/DeBERTa-v3-large-SciFact": ["## Model Description"]}, {"LoneStriker/BioMistral-7B-DARE-AWQ": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-TIES-GPTQ": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"BiMediX/BiMediX-Ara": ["## Model Card for BiMediX-Bilingual", "### Model Details", "### Intended Use", "## Getting Started", "### Training Procedure", "### Model Performance", "### Safety and Ethical Considerations", "### Accessibility", "### Authors"]}, {"antiven0m/speculor-2.7b": []}, {"BMRetriever/BMRetriever-7B": ["## Usage", "## Citation"]}, {"DHEIVER/Medical-mT5-large": ["## How to Get Started with the Model", "## Training Data", "## Evaluation", "### Medical mT5 for Sequence Labelling", "### Single-task supervised F1 scores for Sequence Labelling", "### Multi-task supervised F1 scores for Sequence Labelling", "### Zero-shot F1 scores for Argument Mining. Models have been trained in English and evaluated in Spanish, French and Italian.", "## Ethical Statement", "## Citation"]}, {"bmi-labmedinfo/Igea-1B-v0.0.1": ["## How to use Igea with Hugging Face transformers", "## \ud83d\udea8\u26a0\ufe0f\ud83d\udea8 Bias, Risks, and Limitations \ud83d\udea8\u26a0\ufe0f\ud83d\udea8", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions", "### Recommendations", "## Evaluation", "## Credits"]}, {"Writer/Palmyra-Med-70B-32K": ["## Model Details", "#### Specialized for Biomedical Applications", "### Model Description", "## Intended Use", "### Use with transformers", "## Evaluation Results", "### Performance on Biomedical Benchmarks", "### Medical Use Cases", "### Bias, Risks, and Limitations", "### Citation and Related Information"]}, {"nmitchko/medfalcon-40b-lora": ["## Model Description", "### Architecture", "### Requirements"]}, {"yhyhy3/med-orca-instruct-33b-GPTQ": ["## Model Details", "## Training Details", "### Training Data", "### Training Procedure "]}, {"nmitchko/medfalcon-v2-40b-lora": ["## Model Description", "### Architecture", "### Requirements", "## Training Parameters "]}, {"photonmz/llava-roco-8bit": ["## Model Details", "### Model Description", "### Model Sources", "## Uses", "### Direct Use", "### Downstream Use ", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing ", "#### Training Hyperparameters", "#### Speeds, Sizes, Times", "## Evaluation", "### Testing Data, Factors & Metrics", "### Recommendations", "## Citation"]}, {"vishalkm/medalpaca-7b": ["## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Model Usage", "## Limitations"]}, {"winninghealth/WiNGPT2-7B-Base": ["## WiNGPT2", "## \u4ecb\u7ecd", "## \u7279\u70b9", "## \u5982\u4f55\u4f7f\u7528", "### \u63a8\u7406", "## \u8f93\u51fa\u7ed3\u679c\uff1a\u4f60\u597d\uff01\u4eca\u5929\u6211\u80fd\u4e3a\u4f60\u505a\u4e9b\u4ec0\u4e48\uff1f<|endoftext|>", "### \u63d0\u793a", "### \u4f01\u4e1a\u670d\u52a1", "## \u8bad\u7ec3\u6570\u636e", "## \u6a21\u578b\u5361", "## \u8bc4\u6d4b", "## \u5c40\u9650\u6027\u4e0e\u514d\u8d23\u58f0\u660e", "## \u8bb8\u53ef\u8bc1", "## \u53c2\u8003\u8d44\u6599", "## \u8054\u7cfb\u6211\u4eec"]}, {"keivalya/peft-MedAware": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"justinj92/phi-med-v1": ["## Model Details", "### Model Description", "## Uses", "### Direct Use", "## Bias, Risks, and Limitations", "## Training Details", "### Training Data", "### Training Procedure ", "## Environmental Impact", "## Technical Specifications [optional]", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"Naati101/tb11": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters"]}, {"TheBloke/Dr_Samantha-7B-AWQ": ["## Description", "### About AWQ", "## Repositories available", "## Prompt template: Alpaca", "### Instruction:", "### Response:", "## Provided files, and AWQ parameters", "## How to easily download and use this model in [text-generation-webui](https://github.com/oobabooga/text-generation-webui)", "## Multi-user inference server: vLLM", "### Instruction:", "### Response:", "## Multi-user inference server: Hugging Face Text Generation Inference (TGI)", "### Instruction:", "### Response:", "## Inference from Python code using Transformers", "### Install the necessary packages", "### Transformers example code (requires Transformers 4.35.0 and later)", "### Instruction:", "### Response:", "## Compatibility", "## Discord", "## Thanks, and how to contribute", "## Overview", "## Prompt Template", "### Instruction:", "### Response:", "## OpenLLM Leaderboard Performance", "## Subject-wise Accuracy", "## Evaluation by GPT-4 across 25 random prompts from ChatDoctor-200k Dataset", "### Overall Rating: 83.5/100", "#### Pros:", "#### Cons:"]}, {"TheBloke/Dr_Samantha-7B-GPTQ": ["## Repositories available", "## Prompt template: Alpaca", "### Instruction:", "### Response:", "## Known compatible clients / servers", "## Provided files, and GPTQ parameters", "## How to download, including from branches", "### In text-generation-webui", "### From the command line", "### With `git` (**not** recommended)", "## How to easily download and use this model in [text-generation-webui](https://github.com/oobabooga/text-generation-webui)", "## Serving this model from Text Generation Inference (TGI)", "### Instruction:", "### Response:", "## Python code example: inference from this GPTQ model", "### Install the necessary packages", "### Example Python code", "### Instruction:", "### Response:", "## Compatibility", "## Discord", "## Thanks, and how to contribute", "## Overview", "## Prompt Template", "### Instruction:", "### Response:", "## OpenLLM Leaderboard Performance", "## Subject-wise Accuracy", "## Evaluation by GPT-4 across 25 random prompts from ChatDoctor-200k Dataset", "### Overall Rating: 83.5/100", "#### Pros:", "#### Cons:"]}, {"shrijayan/biogpt": ["## BioGPT", "## Citation"]}, {"moriire/phi-2-healthcare": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"knowledgator/SMILES-DeBERTa-large": ["## Model Details", "### Model Description", "## Citation", "## Model Card Authors", "## Model Card Contact"]}, {"Arvasu/Mental_Health_on_VR": []}, {"clinicalnlplab/finetuned-Llama-2-13b-hf-PubmedQA": []}, {"disi-unibo-nlp/MedGENIE-fid-flan-t5-base-medmcqa": ["## Model description", "## Performance", "### Training hyperparameters", "### Bias, Risk and Limitation", "## Citation"]}, {"cja5553/BJH-perioperative-notes-bioGPT": ["## Dataset ", "## How to use model", "## Codes", "## Citation", "## Questions?"]}, {"LoneStriker/BioMistral-7B-4.0bpw-h6-exl2": []}, {"LoneStriker/BioMistral-7B-5.0bpw-h6-exl2": []}, {"LoneStriker/BioMistral-7B-8.0bpw-h8-exl2": []}, {"LoneStriker/BioMistral-7B-DARE-4.0bpw-h6-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-DARE-6.0bpw-h6-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-TIES-4.0bpw-h6-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-TIES-5.0bpw-h6-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-DARE-GPTQ": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-TIES-6.0bpw-h6-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-TIES-8.0bpw-h8-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"cogbuji/Mr-Grammatology-clinical-problems-Mistral-7B-0.5": ["## Use with mlx", "## Example of use of 1-shot description prompting", "## Question ##", "## Answer ##", "## Question ##"]}, {"pradhaph/medical-falcon-7b": ["## Model Details", "### Model Description", "### Model Sources", "## Uses", "### Direct Use", "### Downstream Use", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "## How to Get Started with the Model"]}, {"abhishek-ch/biomistral-7b-synthetic-ehr": ["## Use with mlx", "## Instructions", "## User Question", "## Loading the model using `mlx`", "## Loading the model using `transformers`"]}, {"orYx-models/Mistral-7b-Lora-Medical-ChatSupport": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact", "### Framework versions"]}, {"hamxea/Mistral-7B-Instruct-v0.2-activity-fine-tuned-v1": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"WinterSiren/Llama-2-7b-chat-finetune": []}, {"LeroyDyer/Mixtral_AI_Medic": []}, {"MDDDDR/bert_base_uncased_NER": ["## Framework versions"]}, {"ZappY-AI/MASHQA-finetuned-Mistral-7B-Instruct": ["### Model Description", "### Dataset Description", "## Prompt template", "### Question:", "### Answer:", "## Basics", "## Technical Specifications", "### Model Architecture and Objective", "## Training", "## How to use", "## Intended Use", "### Direct Use", "### Downstream Use", "#### Out-of-scope Uses", "#### Misuse", "## Intended Users", "### Direct Users"]}, {"LeroyDyer/Mixtral_AI_MasterMind": ["## Concepts : ", "## thoughts and processes : ", "## SelfRAG: ", "## Agent Generation: ", "## Chain of thoughts : ", "## Deep thinking and memory recall: ", "## y-Gene:(assistant Series (chatbots and coders))", "## x-Gene:(Medical Genre)", "## Variant: (competitiion eval sets)", "## Updated DataSets (aligned to under 0.4) <<< All used to create the RAG system! (internal)", "### just to remember its way past merges :", "### Extended capabilities:"]}, {"willoooooooo/medical_Gemma_7b": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"PantagrueLLM/jargon-multidomain-base": ["## Evaluation", "## Using Jargon models with HuggingFace transformers", "## Citation", "### Model Sources [optional]"]}, {"nmitchko/medfalconv2-1a-40b-lora": ["## Model Description", "### Architecture", "### Requirements", "## Training Parameters "]}, {"qanastek/MedAlpaca-LLaMa2-7B": ["## Inference", "## Training procedure", "### Framework versions"]}, {"nihal-tw/finetuned-f7b": ["## Training procedure", "### Framework versions"]}, {"oMarquess/nahara-v1": []}, {"nmitchko/i2b2-querybuilder-codellama-34b": ["## Model Description", "## Prompt Format", "### Instruction:", "### Response:", "### Architecture", "### Requirements", "## Training Parameters ", "## Training procedure", "### Framework versions"]}, {"kislayt/lyme-tweet-classification-v0-llama-2-7b": []}, {"s1ghhh/medllama-2-70b-qlora-4bit": []}, {"phlobo/xmen-fr-ce-medmentions": []}, {"phlobo/xmen-es-ce-medmentions": []}, {"phlobo/xmen-en-ce-medmentions": []}, {"phlobo/xmen-nl-ce-medmentions": []}, {"Tonic/mistralmed": ["## Model Details", "### Model Description", "### Model Sources", "## Uses", "### Direct Use", "### Downstream Use", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Environmental Impact", "## Training Results", "### Model Architecture and Objective", "#### Hardware", "## Model Card Authors [optional]", "## Model Card Contact", "## Training procedure", "### Framework versions"]}, {"pseudolab/K23_MiniMed": ["## \ubaa8\ub378 \uc138\ubd80\uc0ac\ud56d", "### \ubaa8\ub378 \ucd9c\ucc98", "## \uc0ac\uc6a9\ubc95", "### \uc9c1\uc811 \uc0ac\uc6a9", "### \ud558\ub958 \uc0ac\uc6a9", "### \ucd94\ucc9c\uc0ac\ud56d", "## \ud6c8\ub828 \uc138\ubd80\uc0ac\ud56d", "### \ud6c8\ub828 \ub370\uc774\ud130", "### \uacb0\uacfc", "## \ud658\uacbd \uc601\ud5a5", "## \uae30\uc220 \uc0ac\uc591 ", "### \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\uc640 \ubaa9\ud45c", "### \ucef4\ud4e8\ud305 \uc778\ud504\ub77c", "#### \ud558\ub4dc\uc6e8\uc5b4", "#### \uc18c\ud504\ud2b8\uc6e8\uc5b4", "## \ubaa8\ub378 \uce74\ub4dc \uc791\uc131\uc790", "## \ubaa8\ub378 \uce74\ub4dc \uc5f0\ub77d\ucc98", "## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Speeds, Sizes, Times [optional]", "### Results", "#### Summary", "## Environmental Impact", "## Technical Specifications ", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"Laurent1/mpt-7b-instruct2-QLoRa-medical-QA": ["## <b>Model Details</b>", "### Librairies", "### Notebook used for the training", "### Direct Use", "## <b>Bias, Risks, and Limitations</b>", "## <b>Training Details</b>", "### Training Data", "#### Training Hyperparameters", "#### Times"]}, {"hooman650/ct2fast-bge-reranker": ["## Model Details", "### Model Sources", "## Usage", "#### Hardware"]}, {"Mohammed-Altaf/instruct-finetuned-medical-20b-adapters": ["## what this repo is about?", "## Training procedure", "### Framework versions"]}, {"jhliu/ClinicalNoteBERT-small": ["## Overall performance", "## Citation"]}, {"areegtarek/mistral_7b_orca": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact", "## Training procedure", "### Framework versions"]}, {"jtatman/tinymistral-mediqa-248m": []}, {"hamxea/Mistral-7B-v0.1-activity-fine-tuned-v4": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact", "### Framework versions"]}, {"thillaic/MediQuill": ["## Model Details", "### Model Description", "### Model Sources", "## Uses", "#### Hardware", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"LoneStriker/medicine-LLM-13B-5.0bpw-h6-exl2": ["### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"LoneStriker/medicine-LLM-13B-6.0bpw-h6-exl2": ["### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"LoneStriker/medicine-LLM-13B-8.0bpw-h8-exl2": ["### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"ziffir/LiteLLama-460M-1T": ["## Dataset and Tokenization", "## Training Details", "### Using with HuggingFace Transformers", "## Evaluation", "### We evaluate our models on the MMLU task.", "### [Open LLM Leaderboard Evaluation Results](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)", "## Contact"]}, {"NND-project/Clinical_History_Mekkes_PubmedBert": []}, {"LoneStriker/BioMistral-7B-6.0bpw-h6-exl2": []}, {"LoneStriker/BioMistral-7B-SLERP-3.0bpw-h6-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-SLERP-4.0bpw-h6-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-SLERP-5.0bpw-h6-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-SLERP-6.0bpw-h6-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-SLERP-8.0bpw-h8-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-DARE-3.0bpw-h6-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-DARE-5.0bpw-h6-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-TIES-3.0bpw-h6-exl2": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LoneStriker/BioMistral-7B-TIES-AWQ": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"winninghealth/WiNGPT2-14B-Chat-AWQ": ["## WiNGPT2", "## \u66f4\u65b0\u65e5\u5fd7", "## \u4ecb\u7ecd", "## \u7279\u70b9", "## \u5982\u4f55\u4f7f\u7528", "### \u63a8\u7406", "## \u8f93\u51fa\u7ed3\u679c\uff1a\u4f60\u597d\uff01\u4eca\u5929\u6211\u80fd\u4e3a\u4f60\u505a\u4e9b\u4ec0\u4e48\uff1f<|endoftext|>", "### \u63d0\u793a", "### \u4f01\u4e1a\u670d\u52a1", "## \u8bad\u7ec3\u6570\u636e", "## \u6a21\u578b\u5361", "## \u8bc4\u6d4b", "## \u5c40\u9650\u6027\u4e0e\u514d\u8d23\u58f0\u660e", "## \u8bb8\u53ef\u8bc1", "## \u53c2\u8003\u8d44\u6599", "## \u8054\u7cfb\u6211\u4eec"]}, {"solidrust/Hyperion-2.0-Mistral-7B-AWQ": ["## Model Summary", "## How to use", "### Install the necessary packages", "### Example Python code", "### About AWQ", "## Prompt template: ChatML"]}, {"solidrust/NeuralHyperion-2.0-Mistral-7B-AWQ": ["## Model Summary", "## How to use", "### Install the necessary packages", "### Example Python code", "### About AWQ", "## Prompt template: ChatML"]}, {"bartowski/Hercules-4.0-Mistral-v0.2-7B-exl2": ["## Exllama v2 Quantizations of Hercules-4.0-Mistral-v0.2-7B", "## Download instructions"]}, {"LeroyDyer/Mixtral_Cyber_BioMedic": []}, {"nmitchko/dr-niko-70B": ["## Model Details", "### Model Description", "### Examples", "### Model Sources", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact ", "## License - NOMERGE"]}, {"solidrust/Hercules-4.0-Mistral-v0.2-7B-AWQ": ["## Model Summary", "## How to use", "### Install the necessary packages", "### Example Python code", "### About AWQ", "## Prompt template: ChatML"]}, {"LeroyDyer/Mixtral_AI_Multi_Input_Model": ["## CyberSeries: Revolutionizing Language Models", "## CyberSeries", "## Objective: Creating a Blend of Experts!", "## Core Model: Mistral-7B-Instruct-v0.2", "## SUB MODELS - POPULAR ONES "]}, {"Wanxai/afyaLM": []}, {"jcho02/whisper_cleft": ["## Model Details", "### Model Description", "### Model Sources", "## Uses", "### Direct Use", "### Downstream Use ", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "## Recommendations", "## How to Get Started with the Model"]}, {"LeroyDyer/Mixtral_AI_Cyber_MegaMind_1x4": []}, {"LeroyDyer/Mixtral_AI_Cyber_MegaMind_1x4_SFT": ["## CyberSeries", "## Objective: Creating a Blend of Experts!", "## Core Model: Mistral-7B-Instruct-v0.2", "## SUB MODELS - POPULAR ONES "]}, {"bartowski/SlimHercules-4.0-Mistral-7B-v0.2-exl2": ["## Exllama v2 Quantizations of SlimHercules-4.0-Mistral-7B-v0.2", "## Prompt format", "## Available sizes", "## Download instructions"]}, {"solidrust/SlimHercules-4.0-Mistral-7B-v0.2-AWQ": ["## Model Summary", "## How to use", "### Install the necessary packages", "### Example Python code", "### About AWQ", "## Prompt template: ChatML"]}, {"jun10k/Qwen1.5-7B-MeChat": ["## \u5f15\u7528"]}, {"MetaAligner/MetaAligner-IMHI-7B": ["## License"]}, {"vkocaman/JSL-MedMX-7X": ["## \ud83d\udcbb Usage", "## \ud83c\udfc6 Evaluation"]}, {"DHEIVER/ClinicalBERT": ["## Dados de Pr\u00e9-Treinamento", "## Pr\u00e9-Treinamento do Modelo", "### Procedimentos de Pr\u00e9-Treinamento", "### Hiperpar\u00e2metros de Pr\u00e9-Treinamento", "## Como usar o modelo", "## Cita\u00e7\u00e3o"]}, {"fundacionctic/oracle-dermat": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing", "#### Training Hyperparameters", "#### Speeds, Sizes, Times", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors", "## Model Card Contact"]}, {"ilhami/AcademicTranslation2024-tr-to-en": []}, {"TencentMedicalNet/MedicalNet-Resnet10": ["### License", "### Citing MedicalNet", "### Update(2019/07/30)"]}, {"TencentMedicalNet/MedicalNet-Resnet18": ["### License", "### Citing MedicalNet", "### Update(2019/07/30)"]}, {"TencentMedicalNet/MedicalNet-Resnet34": ["### License", "### Citing MedicalNet", "### Update(2019/07/30)"]}, {"TencentMedicalNet/MedicalNet-Resnet50": ["### License", "### Citing MedicalNet", "### Update(2019/07/30)"]}, {"TencentMedicalNet/MedicalNet-Resnet101": ["### License", "### Citing MedicalNet", "### Update(2019/07/30)"]}, {"TencentMedicalNet/MedicalNet-Resnet152": ["### License", "### Citing MedicalNet", "### Update(2019/07/30)"]}, {"TencentMedicalNet/MedicalNet-Resnet200": ["### License", "### Citing MedicalNet", "### Update(2019/07/30)"]}, {"katielink/brats_mri_segmentation_v0.1.0": ["## Workflow", "## Data", "## Training configuration", "## Input", "## Output", "## Model Performance"]}, {"katielink/spleen_ct_segmentation_v0.1.0": ["## Data", "## Training configuration", "## Input and output formats", "## Scores", "## commands example"]}, {"skaliy/spine-segmentation": ["## Requirements", "## Data ", "### Haukeland University Hospital (HUH): 15 T2-weighted MRI scans", "#### Scanner Specifications", "### Sahlgren-ska University Hospital (SUH): 10 T2-weighted MRI scans", "#### Scanner Specifications", "### Data Annotation", "### External Data Evaluation"]}, {"Seiriryu/animefull-final-pruned": []}, {"Dr-BERT/DrBERT-4GB-CP-CamemBERT": ["## 3.1 Install dependencies", "## 3.2 Download NACHOS Dataset text file", "## 3.3 Build your own tokenizer from scratch based on NACHOS", "## 3.4 Preprocessing and tokenization of the dataset", "## 3.5 Model training", "### 3.5.1 Pre-training from scratch", "### 3.5.2 continue pre-training"]}, {"xyla/Clinical-T5-Large": ["### Pretraining Data", "### Note Preprocessing", "### Pretraining Procedures"]}, {"K8778/universe": []}, {"Naskew14/test": []}, {"maliozer/BioGPT-Large": ["## BioGPT"]}, {"Sadiksmart0/unet": []}, {"BrendaTellez/sounds": []}, {"Lilithchouy/xxxx": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"GranataKaoruChigusa/hokkaidomusumetondenheis": []}, {"Monchic/chatwithkani": []}, {"mustafamujahid01/pubmed_model": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "### How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"nyxspirit/2": []}, {"muitmayank5/Best_University_in_Uttar_Pradesh_MUIT_LUCKNOW": []}, {"zl111/ChatDoctor": ["## News", "## Overview", "## Examples:", "## Reference"]}, {"TheEeeeLin/test": []}, {"kroljoza/super-cool-model": []}, {"JaiRiThuX/Senku": []}, {"zee2221/ai_me": []}, {"MiChaelinzo/My_CNN_model": []}, {"SAIASHOK179/Predict_Disease": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"chaithanya13/symptoms": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"Sandeep009/Sandy_Disease_Predection": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"tarak00003/tarakcse": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"SAIASHOK179/Predict_Disease_": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"Narsimha123/Disease_predict_Model": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"sai2002/symtoms": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"boudchicha/soluzione": []}, {"medalpaca/medalpaca-lora-7b-8bit": ["## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Limitations"]}, {"medalpaca/medalpaca-lora-13b-8bit": ["## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Limitations"]}, {"medalpaca/medalpaca-lora-30b-8bit": ["## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Limitations"]}, {"thinksoso/lora-llama-med": []}, {"Izara/TextClassificationOnDiseases": []}, {"medalpaca/medalpaca-lora-7b-16bit": ["## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Limitations"]}, {"CNXT/CHaTx": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"ColtonAi/Llmtrain": []}, {"jjglilleberg/bert-finetuned-ner-nbci-disease": ["## Model description", "## Intended uses & limitations", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"kelly233/test_model": []}, {"Andreas-w/brain-classification": []}, {"Ilangraterol/Dataset_model": []}, {"Rirou360/test": []}, {"duncan93/video": []}, {"rifatul123/Primary_doctor_v1": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model"]}, {"tang11/tang": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"minhnguyen/Advanced_B-cell_Epitopes_Prediction": ["### Method 1: Many-to-One Architecture", "### Method 2: Many-to-Many Architecture"]}, {"MuGeminorum/alexnet-hep2": ["## Maintenance", "## Training Curves", "## Mirror", "## Reference"]}, {"ShabGaming/Brain_MRI_Tumor_Classification": ["## Model", "## GUI", "## Usage", "## Credits"]}, {"ehovel2023/test1": []}, {"erichilarysmithsr/Quality-of-Life-Games": []}, {"BlackBull/yeet": []}, {"Sylvia-my/0517trial": []}, {"weiqso/weiqso": []}, {"TheBloke/medalpaca-13B-GGML": ["## Repositories available", "## THE FILES IN MAIN BRANCH REQUIRES LATEST LLAMA.CPP (May 19th 2023 - commit 2d5db48)!", "## Provided files", "## How to run in `llama.cpp`", "## How to run in `text-generation-webui`", "## Discord", "## Thanks, and how to contribute.", "## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Model Usage", "## Limitations"]}, {"qingfu/demo": []}, {"TheBloke/PMC_LLAMA-7B-GGML": ["## Repositories available", "## Compatibility", "### Original llama.cpp quant methods: `q4_0, q4_1, q5_0, q5_1, q8_0`", "### New k-quant methods: `q2_K, q3_K_S, q3_K_M, q3_K_L, q4_K_S, q4_K_M, q5_K_S, q6_K`", "## Explanation of the new k-quant methods", "## Provided files", "## How to run in `llama.cpp`", "## How to run in `text-generation-webui`", "## Discord", "## Thanks, and how to contribute."]}, {"nmitchko/medguanaco-lora-65b-GPTQ": ["## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Limitations"]}, {"nmitchko/medguanaco-lora-33b-8bit": ["## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Limitations"]}, {"pandas2002/t5_base": []}, {"sirbrentmichaelskoda/Auto-GBT-Dream-Team-Model": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"lovepon/lora-alpaca-med": []}, {"lovepon/lora-alpaca-med-alldata": []}, {"lovepon/lora-llama-literature": []}, {"shibing624/ziya-llama-13b-medical-lora": ["## Training details", "## Usage", "## Usage (HuggingFace Transformers)", "### Inference Examples", "### \u8bad\u7ec3\u6570\u636e\u96c6", "## Citation"]}, {"mroppokhan/Glamare.shopify": []}, {"zhyiyang/chatonco": []}, {"Rongpi007/Lelua": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"tplove2010/test001": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"joey1895/mtzj": []}, {"ahmadnajm/KurdGPT": []}, {"Saurabh1105/MMDet": []}, {"slkoix/ZORO": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"SandeepKanao/BIOBERT_MLMA_HL7-V1": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"yasinelh/retinal_vessel_U-Net": []}, {"skaliy/endometrial_cancer_segmentation": ["## Requirements", "## Usage", "## Results for VIBE", "## Results for multi-sequence (T2, VIBE, and ADC)", "## Results for multi-sequence (T2, VIBE, and ADC) with extra training data (n=54)", "## Support and Contribution"]}, {"khushpreet/eyedisease": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"PledgeVentures/COSMO": []}, {"DunnBC22/vit-large-patch32-384-Breast_Histopathology_Images": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"satzkumar/BoatAI": []}, {"lovepon/lora-bloom-med-bloom": []}, {"Gjesus/VAG": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"FacundoMR/ESESC": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"sindhoorar/brain-tumor": []}, {"deepakkr/bloomed": []}, {"GangCaoLab/U-FISH": []}, {"Minhcdcd/Hjieu": []}, {"AndriLawrence/gpt2-chatkobi-ai": ["## Model Description", "## Model Quality and Limitations", "## Recommended Use Cases", "## How to Use the Model", "### References", "### Potential Bias", "### Developers and Contributions"]}, {"sidmanale643/medLLAMA_3x": ["## Training procedure", "### Framework versions"]}, {"AVS-Net/AVS-Net-Inference": []}, {"monai-test/brats_mri_axial_slices_generative_diffusion": ["#### Example synthetic image", "## Data", "## Training Configuration", "### Training Configuration of Autoencoder", "#### Input", "#### Output", "### Training Configuration of Diffusion Model", "#### Training Input", "#### Training Output", "#### Inference Input", "#### Inference Output", "### Memory Consumption Warning", "## Performance", "#### Training Loss", "## MONAI Bundle Commands", "### Execute Autoencoder Training", "#### Execute Autoencoder Training on single GPU", "#### Override the `train` config to execute multi-GPU training for Autoencoder", "#### Check the Autoencoder Training result", "### Execute Latent Diffusion Model Training", "#### Execute Latent Diffusion Model Training on single GPU", "#### Override the `train` config to execute multi-GPU training for Latent Diffusion Model", "### Execute inference"]}]