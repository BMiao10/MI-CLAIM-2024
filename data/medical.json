[{"monai-test/brats_mri_generative_diffusion": ["#### Example synthetic image", "## Data", "## Training Configuration", "### Training Configuration of Autoencoder", "#### Input", "#### Output", "### Training Configuration of Diffusion Model", "#### Training Input", "#### Training Output", "#### Inference Input", "#### Inference Output", "### Memory Consumption Warning", "## Performance", "#### Training Loss", "## MONAI Bundle Commands", "### Execute Autoencoder Training", "#### Execute Autoencoder Training on single GPU", "#### Override the `train` config to execute multi-GPU training for Autoencoder", "#### Check the Autoencoder Training result", "### Execute Latent Diffusion Training", "#### Execute Latent Diffusion Model Training on single GPU", "#### Override the `train` config to execute multi-GPU training for Latent Diffusion Model", "#### Execute inference", "#### Export checkpoint to TorchScript file"]}, {"monai-test/brats_mri_segmentation": ["## Data", "### Preprocessing", "## Training configuration", "## Input", "## Output", "## Performance", "#### Training Loss and Dice", "#### Validation Dice", "#### TensorRT speedup", "## MONAI Bundle Commands", "#### Execute training:", "#### Override the `train` config to execute multi-GPU training:", "#### Override the `train` config to execute evaluation with the trained model:", "#### Execute inference:", "#### Export checkpoint to TensorRT based models with fp32 or fp16 precision:", "#### Execute inference with the TensorRT model:"]}, {"monai-test/breast_density_classification": []}, {"monai-test/endoscopic_inbody_classification": ["## Data", "### Preprocessing", "## Training configuration", "### Input", "### Output", "## Performance", "#### Training Loss", "#### Validation Accuracy", "#### TensorRT speedup", "## MONAI Bundle Commands", "#### Execute training:", "#### Override the `train` config to execute multi-GPU training:", "#### Override the `train` config to execute evaluation with the trained model:", "#### Execute inference:", "#### Export checkpoint to TorchScript file:", "#### Export checkpoint to TensorRT based models with fp32 or fp16 precision:", "#### Execute inference with the TensorRT model:"]}, {"monai-test/endoscopic_tool_segmentation": ["## Pre-trained weights", "## Data", "### Preprocessing", "## Training configuration", "### Memory Consumption Warning", "### Input", "### Output", "## Performance", "#### Training Loss", "#### Validation IoU", "#### TensorRT speedup", "## MONAI Bundle Commands", "#### Execute training:", "#### Override the `train` config to execute multi-GPU training:", "#### Override the `train` config to execute evaluation with the trained model:", "#### Override the `train` config and `evaluate` config to execute multi-GPU evaluation:", "#### Execute inference:", "#### Export checkpoint to TorchScript file:", "#### Export checkpoint to TensorRT based models with fp32 or fp16 precision:", "#### Execute inference with the TensorRT model:"]}, {"monai-test/lung_nodule_ct_detection": ["## Data", "### 10-fold data splitting", "### Data resampling", "### Data download", "## Training configuration", "### Input", "### Output", "## Performance", "#### Training Loss", "#### Validation Accuracy", "#### TensorRT speedup", "## MONAI Bundle Commands", "#### Execute training:", "#### Override the `train` config to execute evaluation with the trained model:", "#### Execute inference on resampled LUNA16 images by setting `\"whether_raw_luna16\": false` in `inference.json`:", "#### Export checkpoint to TensorRT based models with fp32 or fp16 precision", "#### Execute inference with the TensorRT model"]}, {"monai-test/mednist_gan": ["### Downloading the Dataset", "### Training", "### Inference", "### Export"]}, {"monai-test/mednist_reg": ["## Downloading the Dataset", "## Training", "## Inference", "## Fine-tuning for cross-subject alignments", "## Visualize the first pair of images for debugging (requires `matplotlib`)"]}, {"monai-test/pancreas_ct_dints_segmentation": ["## Data", "### Preprocessing", "## Training configuration", "### Neural Architecture Search Configuration", "### Optimial Architecture Training Configuration", "### Input", "### Output", "### Memory Consumption", "### Memory Consumption Warning", "## Performance", "#### Training Loss", "#### Validation Dice", "#### TensorRT speedup", "### Searched Architecture Visualization", "## MONAI Bundle Commands", "#### Execute model searching:", "#### Execute multi-GPU model searching (recommended):", "#### Execute training:", "#### Override the `train` config to execute multi-GPU training:", "#### Override the `train` config to execute evaluation with the trained model:", "#### Execute inference:", "#### Export checkpoint for TorchScript:", "#### Export checkpoint to TensorRT based models with fp32 or fp16 precision:", "#### Execute inference with the TensorRT model:"]}, {"monai-test/pathology_nuclei_classification": ["## Data", "### Preprocessing", "## Training configuration", "### Memory Consumption Warning", "## Input", "## Output", "## Performance", "#### Training Loss and F1", "#### Validation F1", "## MONAI Bundle Commands", "#### Execute training:", "#### Override the `train` config to execute multi-GPU training:", "#### Override the `train` config to execute evaluation with the trained model:", "#### Override the `train` config and `evaluate` config to execute multi-GPU evaluation:", "#### Execute inference:"]}, {"monai-test/pathology_nuclei_segmentation_classification": ["## Data", "### Preprocessing", "## Training configuration", "### Memory Consumption Warning", "## Input", "## Output", "## Performance", "#### Training Loss and Dice", "#### Validation Dice", "## MONAI Bundle Commands", "#### Execute training, the evaluation during the training were evaluated on patches:", "#### Override the `train` config to execute multi-GPU training:", "#### Override the `train` config to execute evaluation with the trained model, here we evaluated dice from the whole input instead of the patches:", "#### Execute inference:"]}, {"monai-test/pathology_nuclick_annotation": ["## Data", "### Preprocessing", "## Training Configuration", "### Memory Consumption", "### Memory Consumption Warning", "## Input", "## Output", "## Performance", "#### Training Loss and Dice", "#### Validation Dice", "## MONAI Bundle Commands", "#### Execute training:", "#### Override the `train` config to execute multi-GPU training:", "#### Override the `train` config to execute evaluation with the trained model:", "#### Override the `train` config and `evaluate` config to execute multi-GPU evaluation:", "#### Execute inference:"]}, {"monai-test/pathology_tumor_detection": ["## Data", "### Preprocessing", "## Training configuration", "### Pretrained Weights", "### Input", "### Output", "### Inference on a WSI", "### Note on determinism", "## Performance", "## MONAI Bundle Commands", "#### Execute training", "#### Override the `train` config to execute multi-GPU training", "#### Execute inference", "#### Evaluate FROC metric", "#### Export checkpoint to TorchScript file", "#### Export checkpoint to TensorRT based models with fp32 or fp16 precision", "#### Execute inference with the TensorRT model"]}, {"monai-test/prostate_mri_anatomy": ["### **Authors**", "### **Tags**", "## **Model Description**", "## **Data**", "### **Preprocessing**", "#### **Center cropping**", "#### **Resampling**", "## **Performance**", "## **System Configuration**", "## **Limitations** (Optional)", "## **Citation Info** (Optional)", "## **References**"]}, {"monai-test/renalStructures_CECT_segmentation": ["### **Authors**", "### **Tags**", "## **Model Description**", "## **Data**", "#### **Preprocessing**", "## **Performance**", "## **Additional Usage Steps**", "#### Execute training:", "#### Execute training with finetuning", "#### Execute validation:", "#####  Run validation script:", "## **System Configuration**", "## **Limitations**", "## **Citation Info**", "## **References**", "#### **Tests used for bundle checking**"]}, {"monai-test/renalStructures_UNEST_segmentation": ["## Data", "## Method and Network", "## Training configuration", "## Input and output formats", "## Performance", "## commands example", "## More examples output"]}, {"monai-test/spleen_ct_segmentation": ["## Data", "## Training configuration", "### Memory Consumption Warning", "### Input", "### Output", "## Performance", "#### Training Loss", "#### Validation Dice", "#### TensorRT speedup", "## MONAI Bundle Commands", "#### Execute training:", "#### Override the `train` config to execute multi-GPU training:", "#### Override the `train` config to execute evaluation with the trained model:", "#### Override the `train` config and `evaluate` config to execute multi-GPU evaluation:", "#### Execute inference:", "#### Export checkpoint to TensorRT based models with fp32 or fp16 precision:", "#### Execute inference with the TensorRT model:"]}, {"monai-test/swin_unetr_btcv_segmentation": ["## Data", "### Preprocessing", "## Training configuration", "### Memory Consumption", "### Memory Consumption Warning", "### Input", "### Output", "## Performance", "#### Training Loss", "#### Validation Dice", "## MONAI Bundle Commands", "#### Execute training:", "#### Override the `train` config to execute multi-GPU training:", "#### Override the `train` config to execute evaluation with the trained model:", "#### Execute inference:", "#### Export checkpoint to TorchScript file:"]}, {"monai-test/valve_landmarks": ["## Training", "## Inference", "### Reference"]}, {"monai-test/ventricular_short_axis_3label": ["## Data", "## Training", "## Inference"]}, {"monai-test/wholeBody_ct_segmentation": ["### MONAI Label Showcase", "## Data", "### Preprocessing", "## Training Configuration", "## Evaluation Configuration", "### Memory Consumption", "### Memory Consumption Warning", "### Input", "### Output", "## Resource Requirements and Latency Benchmarks", "### GPU Consumption Warning", "### High-Resolution and Low-Resolution Models", "### 1.5 mm (highres) model (Single Model with 104 foreground classes)", "### 3.0 mm (lowres) model (single model with 104 foreground classes)", "## Performance", "### 1.5 mm Model Training", "#### Training Accuracy", "#### Validation Dice", "## MONAI Bundle Commands", "#### Execute training:", "#### Override the `train` config to execute multi-GPU training:", "#### Override the `train` config to execute evaluation with the trained model:", "#### Override the `train` config and `evaluate` config to execute multi-GPU evaluation:", "#### Execute inference:", "#### Execute inference with Data Samples:"]}, {"monai-test/wholeBrainSeg_Large_UNEST_segmentation": ["## Data", "### Important", "## Training configuration", "## Input and output formats", "## commands example", "## More examples output", "## Training/Validation Benchmarking", "## Complete ROI of the whole brain segmentation", "## Bundle Integration in MONAI Lable"]}, {"Orgilbold09/Orikore": []}, {"sidmanale643/medLLAMA22": []}, {"kurugai/llama-ko-medical-chat-7b": ["## Training procedure", "### Framework versions", "## Model Details", "## Intended Use", "## Hardware and Software", "## Training Data", "## Evaluation Results", "## Ethical Considerations and Limitations", "## Reporting Issues", "## Llama Model Index"]}, {"fridriik/mental-health-arg-post-quarantine-covid19-model": ["## Model Details", "### Model Description", "## Uses", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"xtie/LLaMA-LoRA-PET-impression": []}, {"xtie/Alpaca-LoRA-PET-impression": []}, {"Johnyquest7/Endo_Llama-2-7b": ["## Model Architecture", "## Intended Use", "## Limitations", "## Training Data", "## Usage", "## Training procedure", "### Framework versions"]}, {"katielink/your-model-here": []}, {"nmitchko/i2b2-querybuilder-34b-merged": ["## Model Description", "## Prompt Format", "### Instruction:", "### Response:", "### Architecture", "### Requirements", "## Training Parameters ", "## Training procedure", "### Framework versions"]}, {"marcelhuber/KermanyV3_flattened_postprocessed-mirror-auto1-bgcfnc-S": ["## Description", "## Files Included", "## Dataset Used", "## Model Performance", "## Training Details", "## Usage Instructions"]}, {"marcelhuber/KermanyV3_original-mirror-auto1-bgcfnc": ["## Description", "## Files Included", "## Dataset Used", "## Model Performance", "## Training Details", "## Usage Instructions"]}, {"HuzaifaHPC/VGG16_Chest_X_Ray": []}, {"HuzaifaHPC/Resnet_Chest_X_Ray": []}, {"HuzaifaHPC/INCRES_Chest_X_ray_3.h5": []}, {"Sheema/fr-en": []}, {"Techiehill/Himagiri": []}, {"Danielsk88/Hybrid_Segment_Lungs_Model_4_qubits_Simulator": []}, {"ZiaPratama/BrainTumorClassification": []}, {"sedkichayata/unet": []}, {"peopleinsights/pi_poc": []}, {"YuliyaBiletova/_Table_": []}, {"Stephen-smj/LlamaCare": ["## Try LlamaCare"]}, {"flycutter/test": []}, {"npc0/DISC-MedLLM-ggml": ["## How to inference", "### Use it in Python"]}, {"saibala29/medicalAssistant": []}, {"klyang/MentaLLaMA-33B-lora": ["## Other Models in MentaLLaMA", "## Usage", "## License", "## Citation"]}, {"jordypg/PEGembed": []}, {"firiyuu77/llama-2-7b-apeiron": []}, {"linlanio/llmodel-base-city": ["## \u4ecb\u7ecd", "## \u7279\u70b9", "## \u5982\u4f55\u4f7f\u7528", "## \u53c2\u8003\u8d44\u6599", "## \u8054\u7cfb\u6211\u4eec"]}, {"linlanio/llmodel-chat-accompany": ["## \u4ecb\u7ecd", "## \u7279\u70b9", "## \u5982\u4f55\u4f7f\u7528", "## \u53c2\u8003\u8d44\u6599", "## \u8054\u7cfb\u6211\u4eec"]}, {"lvzixin/test": []}, {"katielink/llava-med-7b-slake-delta": ["## Model Description ", "### Model Uses ", "#### Intended Use ", "#### Primary Intended Use ", "#### Out-of-Scope Use ", "### Data ", "### Limitations ", "## Install", "## Serving", "## Evaluation", "### Medical Visual Chat (GPT-assisted Evaluation)", "### Medical VQA", "#### - Prepare Data", "#### - Fine-tuning", "#### - Evaluation", "## Acknowledgement", "## Related Projects"]}, {"Techhacker/Simplar": []}, {"C10H/CS286_BreakHis_CNN": []}, {"NeuraCorp1212/AI_for_SmallMoleculeDesign": []}, {"WIKTO/EMILIA": []}, {"pintt/serum": []}, {"Drbellamy/labrador": ["## Model Details", "### Model Description", "## Uses", "### Direct Use", "### Downstream Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training & Evaluation Details", "## Environmental Impact", "## Citation", "## Model Card Contact"]}, {"terrainternship/eye_hardwriting": []}, {"clinical-adapters/n2c2-pfeiffer-adapter-bert-ast": ["## Clinical adapters / assertion classification", "### Model description", "### How to use the model"]}, {"DaizeDong/GraphsGPT-2W": []}, {"DaizeDong/GraphsGPT-4W": []}, {"areegtarek/FT-Orca-7b": []}, {"ans123/Brain-tumor-segmentor": []}, {"Elysr/TrialMatchLLM": ["## ************************************************", "## \ud83c\udf00 TrialMatchLLM", "## Harnessing LLM for Eligibility Assessment in Clinical Research", "## **Try it:**", "## - **https://trialmatchllm.com/**", "## ************************************************", "## Example Usage"]}, {"TachyHealth/Thealth-phi-2": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Usage", "### Framework versions"]}, {"duynhm/LVM-Med": ["## LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching (Neurips 2023).", "## Table of contents", "## News", "## LVM-Med Pretrained Models", "## Further Training LVM-Med on Large Dataset", "## Prerequisites", "## Preparing datasets", "### For the Brain Tumor Dataset", "### For VinDr", "### Others", "## Downstream Tasks", "### Segmentation", "### 1. End-to-End Segmentation", "#### ResNet-50 version", "### 2. Prompt-based Segmentation with ViT-B", "#### Train", "#### Inference", "#### Train", "#### Test", "### Image Classification", "### Object Detection", "## Citation", "## Related Work", "## License"]}, {"rootstrap-org/Alzheimer-Classifier-Demo": ["### Model Description", "## Alzheimer Classifier Model", "### Model Sources ", "## Uses", "### Direct Use", "## Bias, Risks, and Limitations", "## Training Details", "### Training Data", "### Training Procedure", "## Evaluation and Results"]}, {"TachyHealth/Thealth_Mixtral-8x7B": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"BOBBYBEAR1/BIOB": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"Lipu124/Spd1": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"Aiaxis/SmallDisMedLM": []}, {"jason954/diabetisdevices": []}, {"vaishakgkumar/stablemedv1": []}, {"SaranDharshan/Ayumaton": ["## Overview", "## Fine-tuning Details", "## Disclaimer", "## License", "## Contact"]}, {"CravenMcin22/Cherub": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"bryceschultz/CataractsDetector": []}, {"Juanfco/Juanpancho": []}, {"KidsWithTokens/Med-SAM-Adapter": ["## A Quick Overview ", "## Run on  your own dataset"]}, {"KidsWithTokens/Medical-Adapter-Zoo": ["## Med-Adpt Zoo Map \ud83d\udc18\ud83d\udc0a\ud83e\udd8d\ud83e\udd92\ud83e\udda8\ud83e\udd9c\ud83e\udda5", "## What", "## Why", "## How to Use", "## Authorship"]}, {"Kazilsky/Petal_Model": []}, {"Sebastiangor/skibidi": []}, {"ijintran/test": []}, {"darklord25/fewshot_random_subspace": ["## Overview", "## Project Summary", "## Contact"]}, {"Aryadha/Policy_summariser": []}, {"subek/brain_tumor_segmentation": []}, {"bflooreonline/yesyoucan": []}, {"rrawen/Digitalfriend": []}, {"davanstrien/flair": ["## FLAIR: A Foundation LAnguage Image model of the Retina", "## Install FLAIR", "## Usage", "## **Note**: problems during automatic **pre-trained weights download**", "## Pre-training and transferability", "### \ud83d\udce6 Foundation model pre-training", "### \ud83d\udce6 Transferability to downstream tasks/domains"]}, {"bulentnuran/Votran": []}, {"Linguist/HealGPT": ["### Description and Details"]}, {"TachyHealth/Thealth-llama-70b": []}, {"antrikxh/Efficient-AD": ["## What Sets Efficient-AD Apart?", "## Key Features:", "## Performance Overview:", "## How to Use Efficient-AD:"]}, {"jinee/note": ["## Model Description", "## Model Sources", "## Usage", "## Dataset", "## Training and Hyper-parameters", "### List of LoRA config ", "### List of Training arguments", "### Experimental setup", "## Limitations", "## Non-commercial use", "## INMED DATA"]}, {"Goodnight7/finetune_distilbert_squad": []}, {"janong24/snoringDetectorAI": []}, {"seth-zou/SethModel01": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"irlab-udc/MindWell": ["## How to Get Started with MindWell", "## Training", "#### Configurations and Hyperparameters", "#### Framework versions", "## Environmental Impact"]}, {"exzread/MIT-AI": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"wdh9563/MedicalQA-BERT": []}, {"anezatra/breast-cancer-wisconsin": ["### Model Description"]}, {"bala1524/Drug_Comb_Pre_Mistral": []}, {"Juanfco/Jotaefe": []}, {"ayushnoori/alive": ["## Training Dashboard"]}, {"cja5553/BJH-perioperative-notes-bioClinicalBERT": ["## Dataset", "## How to use model", "## Codes", "## Citation", "## Questions?"]}, {"TachyHealth/Gazal-70b": []}, {"preetam-llm/answer_the_unanswered": []}, {"InMedData/InMD-X-CAR": ["## InMD-X: Large Language Models for Internal Medicine Doctors", "### Model Description", "### Model Sources [optional]", "## Uses", "### List of LoRA config ", "### List of Training arguments", "### Experimental setup", "## Limitations", "## Non-commercial use", "## INMED DATA"]}, {"InMedData/InMD-X-RHE": ["## InMD-X: Large Language Models for Internal Medicine Doctors", "### Model Description", "### Model Sources [optional]", "## Uses", "### List of LoRA config ", "### List of Training arguments", "### Experimental setup", "## Limitations", "## Non-commercial use", "## INMED DATA"]}, {"InMedData/InMD-X-GAS": ["## InMD-X: Large Language Models for Internal Medicine Doctors", "### Model Description", "### Model Sources [optional]", "## Uses", "### List of LoRA config ", "### List of Training arguments", "### Experimental setup", "## Limitations", "## Non-commercial use", "## INMED DATA"]}, {"InMedData/InMD-X-ALL": ["## InMD-X: Large Language Models for Internal Medicine Doctors", "### Model Description", "### Model Sources [optional]", "## Uses", "### List of LoRA config ", "### List of Training arguments", "### Experimental setup", "## Limitations", "## Non-commercial use", "## INMED DATA"]}, {"InMedData/InMD-X-MET": ["## InMD-X: Large Language Models for Internal Medicine Doctors", "### Model Description", "### Model Sources [optional]", "## Uses", "### List of LoRA config ", "### List of Training arguments", "### Experimental setup", "## Limitations", "## Non-commercial use", "## INMED DATA"]}, {"InMedData/InMD-X-ONC": ["## InMD-X: Large Language Models for Internal Medicine Doctors", "### Model Description", "### Model Sources [optional]", "## Uses", "### List of LoRA config ", "### List of Training arguments", "### Experimental setup", "## Limitations", "## Non-commercial use", "## INMED DATA"]}, {"InMedData/InMD-X-MED": ["## InMD-X: Large Language Models for Internal Medicine Doctors", "### Model Description", "### Model Sources [optional]", "## Uses", "### List of LoRA config ", "### List of Training arguments", "### Experimental setup", "## Limitations", "## Non-commercial use", "## INMED DATA"]}, {"InMedData/InMD-X-URO": ["## InMD-X: Large Language Models for Internal Medicine Doctors", "### Model Description", "### Model Sources [optional]", "## Uses", "### List of LoRA config ", "### List of Training arguments", "### Experimental setup", "## Limitations", "## Non-commercial use", "## INMED DATA"]}, {"InMedData/InMD-X-RES": ["## InMD-X: Large Language Models for Internal Medicine Doctors", "### Model Description", "### Model Sources [optional]", "## Uses", "### List of LoRA config ", "### List of Training arguments", "### Experimental setup", "## Limitations", "## Non-commercial use", "## INMED DATA"]}, {"InMedData/InMD-X-HEM": ["## InMD-X: Large Language Models for Internal Medicine Doctors", "### Model Description", "### Model Sources [optional]", "## Uses", "### List of LoRA config ", "### List of Training arguments", "### Experimental setup", "## Limitations", "## Non-commercial use", "## INMED DATA"]}, {"InMedData/InMD-X-INF": ["## InMD-X: Large Language Models for Internal Medicine Doctors", "### Model Description", "### Model Sources [optional]", "## Uses", "### List of LoRA config ", "### List of Training arguments", "### Experimental setup", "## Limitations", "## Non-commercial use", "## INMED DATA"]}, {"LoneStriker/BioMistral-7B-SLERP-AWQ": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"bartowski/Hercules-3.1-Mistral-7B-exl2": ["## Exllama v2 Quantizations of Hercules-3.1-Mistral-7B", "## Download instructions"]}, {"eugeneframesi/Morphosis": []}, {"shaikatasif/BioMistral-7B-llamafile": ["## GGUF version", "## [From the original model card ](https://huggingface.co/BioMistral/BioMistral-7B/)"]}, {"Srijinesh/BERT-PUBMEDQA": []}, {"antoniomae/XTTS_V2_CPU_working": []}, {"StanfordShahLab/motor-t-base": ["## Model Details", "### Model Description", "### Model Sources", "## Uses", "### Direct Use", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing", "#### Training Hyperparameters", "## Evaluation", "## Technical Specifications", "## Citation", "## Model Card Authors", "## Model Card Contact"]}, {"bartowski/BioMistral-7B-exl2": ["## Exllama v2 Quantizations of BioMistral-7B", "## Download instructions"]}, {"unreal-hug/segformer-b4-seed17-feb-28-v2": []}, {"razaobj/ttc": []}, {"DracolIA/GPT-2-LoRA-HealthCare": ["## GPT-2-LoRA-HealthCare", "## [Spaces Demo](https://huggingface.co/spaces/FFatih/SAE-GPT2-PROD)", "## Description : GPT-2-LoRA-HealthCare"]}, {"vsocrates/incar-status-any": ["## Model Details", "### Model Description", "## Uses", "## Bias, Risks, and Limitations", "## Training Details", "### Training Data", "### Training Procedure", "## Evaluation", "### Testing Data, Factors & Metrics", "### Results", "## Citation [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"escom/depression-anxiety": []}, {"bartowski/Hyperion-2.0-Mistral-7B-exl2": ["## Exllama v2 Quantizations of Hyperion-2.0-Mistral-7B", "## Download instructions"]}, {"bartowski/NeuralHyperion-2.0-Mistral-7B-exl2": ["## Exllama v2 Quantizations of NeuralHyperion-2.0-Mistral-7B", "## Download instructions"]}, {"snehalsapkale/test_model": []}, {"KarthikSaran/text_classification_FlanT5": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"MLMan21/MishraShayeSkinCancerModel": ["### Model Description"]}, {"Allaine/face-mask-detection": ["## Model Details"]}, {"Serega1200/test": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"bartowski/Hyperion-3.0-Mistral-7B-alpha-exl2": ["## Exllama v2 Quantizations of Hyperion-3.0-Mistral-7B-alpha", "## Download instructions"]}, {"Dijitaal/DijiHax.Spooky.Pi": ["## Model Details", "### Model Description", "### Model Sources", "## Uses", "### Direct Use", "### Downstream Use", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing", "#### Training Hyperparameters", "#### Speeds, Sizes, Times", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "## Model Examination", "## Environmental Impact", "## Technical Specifications", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation", "## Glossary", "## More Information", "## Model Card Authors", "## Model Card Contact"]}, {"LeroyDyer/Mixtral_AI_CyberRolePlay": []}, {"bartowski/Hyperion-3.0-Yi-34B-exl2": ["## Exllama v2 Quantizations of Hyperion-3.0-Yi-34B", "## Download instructions"]}, {"pfriedri/wdm-3d": ["## Origial GitHub repository", "## Pre-trained models", "### BraTS 2023 (T1-weighted brain MR image generation)", "### LIDC-IDRI (Lung CT image generation)", "## Hardware requirements", "## Citation"]}, {"LeroyDyer/Mixtral_AI_LongTalker": ["### Models Merged"]}, {"Yeraz22/SpeakerPhoto": []}, {"a7pute/llm_medicalQA": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"xumeng/modeltest": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations"]}, {"NCI-all-topics/Rad_topic_modeling.ipynb": []}, {"ash1kn1zar/ICD_10_Finder": ["## Uses", "### Direct Use", "### Training Data", "#### Testing Data", "## Model Card Contact"]}, {"Paulie-Aditya/sign-language-detection": ["## Stacked Classifier: RF + SVM + XGB", "## Stacked Classifier: RF + SVM + KNN + XGB"]}, {"ShayanGerami/Drug_review_sentiment_model": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"nmitchko/dr-niko-lora-70B": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact ", "## License - NOMERGE"]}, {"harshinde/Covid-19_Vaccines_Analysis": []}, {"hnthnh/BCI_PyramidPix2Pix": []}, {"sarth123/dbrdetectionmodel": []}, {"bartowski/medalpaca-7b-exl2": ["## Exllama v2 Quantizations of medalpaca-7b", "## Download instructions"]}, {"bartowski/Hercules-4.0-Yi-34B-exl2": ["## Exllama v2 Quantizations of Hercules-4.0-Yi-34B", "## Download instructions"]}, {"Shekswess/llama-2-7b-chat-bnb-4bit-medical": []}, {"Shekswess/gemma-1.1-7b-it-bnb-4bit-medical": []}, {"aistrosight/Neg-CamemBERT-bio": ["## Model Details", "### Training Data", "### Fine-tuning", "### Evaluation", "### Results ", "### Model Description", "### Direct Use", "#### Limitations and bias"]}, {"Shekswess/mistral-7b-instruct-v0.2-bnb-4bit-medical": []}, {"Arogeneration/Mouhu-0.1": ["## \u4eca\u73fe\u5728\u306f\u30e2\u30c7\u30eb\u3092\u958b\u767a\u4e2d\u3067\u3059\uff01", "## \u30e2\u30c7\u30eb\u306e\u8a73\u7d30", "## Mouhu-0.1\u306e\u5229\u7528\u898f\u7d04", "### \u30e2\u30c7\u30eb\u306e\u8aac\u660e", "## \u7528\u9014", "## \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u8a73\u7d30", "### \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf", "### \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u624b\u9806", "#### \u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406 [\u30aa\u30d7\u30b7\u30e7\u30f3]", "#### \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf", "#### \u901f\u5ea6\u3001\u30b5\u30a4\u30ba\u3001\u6642\u9593 [\u30aa\u30d7\u30b7\u30e7\u30f3]", "## \u8a55\u4fa1", "### \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3001\u8981\u56e0\u3001\u304a\u3088\u3073\u30e1\u30c8\u30ea\u30c3\u30af\u30b9", "#### \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf", "#### \u8981\u56e0", "#### \u30e1\u30c8\u30ea\u30c3\u30af\u30b9", "### \u7d50\u679c", "#### \u30b5\u30de\u30ea\u30fc", "## \u30e2\u30c7\u30eb\u306e\u691c\u67fb [\u30aa\u30d7\u30b7\u30e7\u30f3]", "## \u6280\u8853\u4ed5\u69d8 [\u30aa\u30d7\u30b7\u30e7\u30f3]", "### \u30e2\u30c7\u30eb\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3068\u76ee\u7684", "### \u30b3\u30f3\u30d4\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u30a4\u30f3\u30d5\u30e9", "#### \u30cf\u30fc\u30c9\u30a6\u30a7\u30a2", "#### \u30bd\u30d5\u30c8\u30a6\u30a7\u30a2"]}, {"Shekswess/llama-3-8b-Instruct-bnb-4bit-medical": []}, {"TachyHealth/Gazal_8b_llama_3_Med_finetuned": []}, {"jethro682/pretrain_mri": []}, {"Stanlito/llama-2-7b-stanlito": ["## \ud83d\udd27 Training", "## \ud83d\udcbb Usage"]}, {"NouRed/BioMed-Tuned-Llama-3-8b": ["## Model Details", "## \u2699\ufe0f Config", "## \ud83d\udcbb Usage", "### Instruction:", "### Input:", "### Response:", "### Instruction:", "### Response:", "## \ud83d\udccb Cite Us"]}, {"solidrust/internistai-base-7b-v0.2-AWQ": ["## Model Summary", "## How to use", "### Install the necessary packages", "### Example Python code", "### About AWQ"]}, {"Zeal-Nir/dr-handwriting-keras-model": []}, {"MetaAligner/MetaAligner-IMHI-13B": ["## License"]}, {"geovanio/caoselvagem": []}, {"smarttiger/ipcamera": ["## Usage", "## Main Results", "### Zero-Shot", "### LLM360", "### OpenLLM Leaderboard", "## Evaluation", "### Setup", "### Evaluate OpenELM", "## Bias, Risks, and Limitations", "## Citation"]}, {"makuo12/segment": []}, {"Roselia-penguin/medical_llama2_7b": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"asfASFasdfa/TroutLM": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"PantagrueLLM/jargon-biomed": ["## Evaluation", "## Using Jargon models with HuggingFace transformers", "## Citation", "### Model Sources [optional]"]}, {"PantagrueLLM/jargon-NACHOS": ["## Evaluation", "## Using Jargon models with HuggingFace transformers", "## Citation", "### Model Sources [optional]"]}, {"DHEIVER/MedicalSummarization": ["## Model Description", "## Intended Uses & Limitations", "### Intended Uses", "### How to Use"]}, {"EnjoyCodeX/MedLang-13B": ["## Model Details", "### Model Description", "## How to Get Started with the Model", "## Training Details", "### Training Data", "## Evaluation", "#### Testing Data", "### Model Architecture", "### Recommendations", "#### GPU", "## More Information [optional]", "## Model Card Authors [optional]"]}, {"JuanPablo4to/CoachPRODI": []}, {"JosefAlbers/phi-3-medmcqa-openbiollm": []}, {"Obotu/test": []}, {"Bon-God/ChemboC": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"amiguel/lightining_studio": []}, {"safehavens/safehavens_chatbot": ["## Overview", "## Features", "## Training Dataset", "## Ethical Considerations", "### Confidentiality and Privacy", "### Bias and Fairness", "### Risk Factors", "## Usage", "## Contributions"]}, {"randomani/Llama-2-7b-chat-Medchat-finetune": ["## Model and Dataset", "## QLoRA Parameters", "## bitsandbytes Parameters", "## Training Arguments", "## Supervised Fine-Tuning (SFT) Parameters", "## References"]}, {"restufiqih/coba_model": []}, {"Lab-Rasool/SeNMo": []}, {"alexbene/BianqueNet": ["## Intended uses & limitations", "## BibTeX entry and citation info"]}, {"bmi-labmedinfo/Igea-350M-v0.0.1": ["## How to use Igea with Hugging Face transformers", "## \ud83d\udea8\u26a0\ufe0f\ud83d\udea8 Bias, Risks, and Limitations \ud83d\udea8\u26a0\ufe0f\ud83d\udea8", "### Recommendations", "## Evaluation", "## Credits"]}, {"data-is-everything/journal-retrieval-code": ["## Model Description", "## Training Data", "## Training Procedure", "## Evaluation", "## Intended Use Cases", "## Limitations", "## How to Use", "### Installation"]}, {"fundacionctic/vanilla-dermat": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"fundacionctic/predict-dermat": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing", "#### Training Hyperparameters", "#### Speeds, Sizes, Times", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors", "## Model Card Contact"]}, {"adevbanshi/ner_medical_coding": []}, {"bilguunagii/medical_model": []}, {"hanzohazashi1/symptoms_disease": []}, {"hanzohazashi1/physician_model": []}, {"Mohamedfadil369/brainsait": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model"]}, {"azminetoushikwasi/NeuralCGMM": ["## Architecture", "## Citation"]}, {"azminetoushikwasi/Drug-Classification-NLP": []}, {"clinicalnlplab/me-llama": ["## Model Overview", "## Pretraining and Data", "## Evaluation", "### Performance", "## Model Details", "## Usage"]}]