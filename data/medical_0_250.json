[{"bvanaken/clinical-assertion-negation-bert": ["## Model description", "#### How to use the model", "### Cite"]}, {"medicalai/ClinicalBERT": ["## Pretraining Data", "## Model Pretraining", "### Pretraining Procedures", "### Pretraining Hyperparameters", "## How to use the model", "## Citation"]}, {"zhihan1996/DNABERT-2-117M": []}, {"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224": ["## Citation", "## Model Use", "### How to use", "### Intended Use", "#### Primary Intended Use", "#### Out-of-Scope Use", "## Data", "## Limitations", "## Further information"]}, {"pszemraj/long-t5-tglobal-base-sci-simplify-elife": ["## Model description", "## Usage ", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Eval results", "### Training hyperparameters", "### Training results"]}, {"starmpcc/Asclepius-Llama2-7B": ["## UPDATE", "### 2024.01.10", "## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Training Hyperparameters", "#### Speeds, Sizes, Times", "## Citation"]}, {"owkin/phikon": ["### Model Description", "## Uses", "### Direct Use", "### Downstream Use ", "## Technical Specifications", "### Compute Infrastructure", "### Hardware", "### Software", "### BibTeX entry and citation info"]}, {"wisdomik/QuiltNet-B-16-PMB": ["## QuiltNet-B-16-PMB Description", "## Direct Use", "## Downstream Use", "### Intended Use", "#### Primary intended uses", "### Out-of-Scope Use Cases", "## Training Data"]}, {"Clinical-AI-Apollo/Medical-NER": ["## Model description", "### Training hyperparameters", "## Usage", "### Author", "### Framework versions"]}, {"BioMistral/BioMistral-7B": []}, {"microsoft/BioGPT-Large-PubMedQA": ["## BioGPT", "## Citation"]}, {"FremyCompany/BioLORD-2023": ["## Sibling models", "## Training strategy", "### Summary of the 3 phases", "### Contrastive phase: details", "### Self-distallation phase: details", "## Citation", "## Usage (Sentence-Transformers)", "## Usage (HuggingFace Transformers)", "## License"]}, {"prov-gigapath/prov-gigapath": ["## A whole-slide foundation model for digital pathology from real-world data", "## Model Overview", "## Install", "## Model Download", "## Inference", "### Inference with the tile encoder", "### Inference with the slide encoder", "## Fine-tuning", "### Tile-Level Linear Probing Example Using PCam Dataset", "### Slide-Level Fine-Tuning Example Using PANDA Dataset", "## Sample Data Download", "## Model Uses", "### Intended Use", "### Primary Intended Use", "### Out-of-Scope Use", "## Usage and License Notices", "## Acknowledgements", "## Citation"]}, {"knowledgator/SMILES2IUPAC-canonical-base": ["## Model Details", "### Model Description", "### Model Sources", "## Quickstart", "### SMILES to IUPAC", "#### ! Preferred IUPAC style", "#### To perform simple translation, follow the example:", "#### Processing in batches:", "#### Validation SMILES to IUPAC translations", "## Bias, Risks, and Limitations", "### Training Procedure", "## Evaluation", "## Citation", "## Model Card Authors", "## Model Card Contact"]}, {"Severus27/BeingWell_llama2_7b": []}, {"medalpaca/medalpaca-7b": ["## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Model Usage", "## Limitations"]}, {"alchemab/antiberta2": ["## Example usage"]}, {"aaditya/Llama3-OpenBioLLM-70B": ["### Use with transformers", "## **Training procedure**", "### **Training hyperparameters**", "### **Peft hyperparameters**", "### **Training results**", "### **Framework versions**", "## Detailed Medical Subjectwise accuracy"]}, {"ruslanmv/Medical-Llama3-8B": []}, {"AdaptLLM/medicine-chat": ["### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## [Open LLM Leaderboard Evaluation Results](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)", "## Citation"]}, {"Technoculture/MT7Bi-sft": ["## Open LLM Leaderboard", "### Model Evaluation Benchmark", "### ARC: 54.1%", "### HellaSwag: 75.11%", "### TruthfulQA: 43.08%", "### Winogrande: 72.14%", "### GSM8K: 15.54%"]}, {"klyang/MentaLLaMA-chat-7B": ["## Other Models in MentaLLaMA", "## Usage", "## License", "## Citation"]}, {"mradermacher/Dr.Samantha-8B-i1-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"BioMistral/BioMistral-7B-DARE": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"HPAI-BSC/Llama3-Aloe-8B-Alpha": ["## Model Details", "### [](https://huggingface.co/templates/model-card-example#model-description)Model Description", "### [](https://huggingface.co/templates/model-card-example#model-sources-optional)Model Sources [optional]", "## Model Performance", "## Uses", "### Direct Use", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "#### Transformers pipeline", "#### Transformers AutoModelForCausalLM", "## Training Details", "### Training Data", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Metrics", "### Results", "#### Summary", "## Environmental Impact", "## Model Card Authors", "## Model Card Contact", "## Citations"]}, {"Writer/palmyra-med-20b": ["## Model description", "### Model Source", "## Uses", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## Usage ", "## output ##", "##  Dataset", "## Evaluation", "## Limitation", "## Citation and Related Information", "## Contact"]}, {"Mohammed-Altaf/Medical-ChatBot": ["## Quickstart", "## Example Outputs"]}, {"mradermacher/JSL-MedLlama-3-8B-v9-i1-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"johnsnowlabs/JSL-MedLlama-3-8B-v2.0": ["## \ud83d\udcbb Usage", "## \ud83c\udfc6 Evaluation"]}, {"flaviagiammarino/pubmed-clip-vit-base-patch32": ["## Model Description", "## Usage", "## Additional Information", "### Licensing Information", "### Citation Information"]}, {"TheBloke/Dr_Samantha-7B-GGUF": ["## Description", "### About GGUF", "## Repositories available", "## Prompt template: Alpaca", "### Instruction:", "### Response:", "## Compatibility", "## Explanation of quantisation methods", "## Provided files", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using llama-cpp-python", "#### First install the package", "#### Simple llama-cpp-python example code", "## How to use with LangChain", "## Discord", "## Thanks, and how to contribute", "## Overview", "## Prompt Template", "### Instruction:", "### Response:", "## OpenLLM Leaderboard Performance", "## Subject-wise Accuracy", "## Evaluation by GPT-4 across 25 random prompts from ChatDoctor-200k Dataset", "### Overall Rating: 83.5/100", "#### Pros:", "#### Cons:"]}, {"unikei/distilbert-base-re-punctuate": ["## How to use it in your code:"]}, {"Locutusque/Hercules-3.1-Mistral-7B": ["## Model Description", "## Intended Uses & Potential Bias", "## Limitations and Risks", "## Training Data", "## Training Procedure"]}, {"starmpcc/Asclepius-Llama2-13B": ["## UPDATE", "### 2024.01.10", "## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Training Hyperparameters", "#### Speeds, Sizes, Times", "## Citation"]}, {"chaoyi-wu/PMC_LLAMA_7B": []}, {"health360/Healix-1.1B-V1-Chat-dDPO": ["## Model Description", "## Training Data", "## Intended Use", "## Limitations", "## Performance", "## Ethical Considerations", "## Papers", "### Input Format"]}, {"winninghealth/WiNGPT2-Llama-3-8B-Base": ["## WiNGPT2", "## \u66f4\u65b0\u65e5\u5fd7", "## \u5982\u4f55\u4f7f\u7528", "### \u63a8\u7406", "## \u8f93\u51fa\u7ed3\u679c\uff1a\u4f60\u597d\uff01\u4eca\u5929\u6211\u80fd\u4e3a\u4f60\u505a\u4e9b\u4ec0\u4e48\uff1f<|end_of_text|>", "### \u63d0\u793a", "## \u6a21\u578b\u5361", "####  \u8bad\u7ec3\u914d\u7f6e\u4e0e\u53c2\u6570", "#### \u8bad\u7ec3\u6570\u636e", "## \u4e2d\u6587\u533b\u7597\u8bc4\u6d4b - WiNEval", "### \u4f01\u4e1a\u670d\u52a1", "## \u5c40\u9650\u6027\u4e0e\u514d\u8d23\u58f0\u660e", "## \u8bb8\u53ef\u8bc1", "## \u8054\u7cfb\u6211\u4eec"]}, {"johnsnowlabs/JSL-Med-Sft-Llama-3-8B": ["## \ud83d\udcbb Usage", "## \ud83c\udfc6 Evaluation"]}, {"flaviagiammarino/medsam-vit-base": ["## Model Description", "## Usage", "## Additional Information", "### Licensing Information", "### Citation Information"]}, {"LeroyDyer/Mixtral_AI_Cyber_2.0": ["## Merge Details", "### Merge Method", "### Models Merged", "## Nous-Yarn-Mistral-7b-128k ", "## Severian/Nexus-IKM-Mistral-7B-Pytorch", "## METHOD 2", "## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"Falconsai/medical_summarization": ["## Model Description", "## Intended Uses & Limitations", "### Intended Uses", "### How to Use"]}, {"yhyhy3/med-orca-instruct-33b": ["## Model Details", "## Training Details", "### Training Data", "### Training Procedure "]}, {"chaoyi-wu/MedLLaMA_13B": []}, {"microsoft/llava-med-v1.5-mistral-7b": ["## License", "## Intended use", "### Primary Intended Use", "### Out-of-Scope Use", "## Data", "## How to use", "## Limitations", "### BibTeX entry and citation info"]}, {"health360/Healix-3B": []}, {"yhyhy3/open_llama_7b_v2_med_instruct": ["## Model Details", "## How to Get Started with the Model", "### Input: What is the capital of New Jersey?", "### Response:'''", "## Training Details", "### Training Data", "### Training Procedure "]}, {"jilp00/Hermes-2-SOLAR-10.7B-Symbolic": []}, {"johnsnowlabs/JSL-MedMNX-7B-v2.0": ["## \ud83d\udcbb Usage", "## \ud83c\udfc6 Evaluation"]}, {"squarelike/llama2-ko-medical-7b": ["## \ud559\uc2b5 \ub370\uc774\ud130", "## \ud559\uc2b5"]}, {"epfl-llm/meditron-70b": ["## Model Details", "### Model Sources", "## Uses", "### Direct Use", "### Downstream Use", "### Out-of-Scope Use", "## Truthfulness, Helpfulness, Risk, and Bias", "### Recommendations", "## Training Details", "### Training Data", "#### Data Preprocessing", "### Training Procedure ", "#### Training Hyperparameters", "#### Speeds, Sizes, Times", "## Evaluation", "### Testing Data & Metrics", "#### Testing Data", "#### Metrics", "### Results", "## Environmental Impact", "## Citation"]}, {"winninghealth/WiNGPT2-Llama-3-8B-Chat": ["## WiNGPT2", "## \u66f4\u65b0\u65e5\u5fd7", "## \u5982\u4f55\u4f7f\u7528", "### \u63a8\u7406", "## \u8f93\u51fa\u7ed3\u679c\uff1a\u4f60\u597d\uff01\u4eca\u5929\u6211\u80fd\u4e3a\u4f60\u505a\u4e9b\u4ec0\u4e48\uff1f<|end_of_text|>", "### \u63d0\u793a", "## \u6a21\u578b\u5361", "####  \u8bad\u7ec3\u914d\u7f6e\u4e0e\u53c2\u6570", "#### \u8bad\u7ec3\u6570\u636e", "## \u4e2d\u6587\u533b\u7597\u8bc4\u6d4b - WiNEval", "### \u4f01\u4e1a\u670d\u52a1", "## \u5c40\u9650\u6027\u4e0e\u514d\u8d23\u58f0\u660e", "## \u8bb8\u53ef\u8bc1", "## \u8054\u7cfb\u6211\u4eec"]}, {"LeroyDyer/Mixtral_AI_Cyber_4.0": ["### Models Merged", "### Configuration"]}, {"Locutusque/Hyperion-2.0-Mistral-7B": ["## Model Details", "## Model Description", "## Intended Use", "## Training Data", "## Quants", "## Evaluation Results", "## How to Use", "## Known Limitations", "## Licensing Information"]}, {"mradermacher/MedLLaMA-3-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"BioMistral/BioMistral-7B-TIES": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LeroyDyer/Mixtral_AI_Cyber_5.0": ["## LeroyDyer/Mixtral_AI_Cyber 5_7b"]}, {"MaziyarPanahi/Bioxtral-4x7B-v0.1": ["## Description", "## How to use it", "## Quantized mode", "## Examples", "## Eval"]}, {"hamxea/Mistral-7B-v0.1-activity-fine-tuned-v5": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"Obrolin/Kesehatan-7B-v0.1": ["## Obrolin Kesehatan!", "## System Prompt (Optional) :", "## Output Example :", "## Still in alpha build, don't expect perfection just yet :)", "## License", "## Based on [azale-ai/Starstreak-7b-beta](https://huggingface.co/azale-ai/Starstreak-7b-beta)!", "## Citation"]}, {"hamxea/Llama-2-7b-chat-hf-activity-fine-tuned-v4": ["## Model details", "## Intended use", "## Factors", "## Metrics", "## Evaluation datasets", "## Training dataset", "## Quantitative analysis", "## Ethical considerations"]}, {"mradermacher/Dr.Samantha-8B-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"LeroyDyer/Mixtral_BioMedical": []}, {"Locutusque/Hercules-4.0-Yi-34B": ["## Model Description", "## Intended Uses & Potential Bias", "## Limitations and Risks", "## Training Procedure", "## Evaluation"]}, {"Locutusque/NeuralHyperion-2.0-Mistral-7B": ["## Model Details", "## Model Description", "## Intended Use", "## Training Data", "## Evaluation Results", "## Quants", "## How to Use", "## Known Limitations", "## Licensing Information"]}, {"Sharathhebbar24/Med_GPT2": ["## Model description", "### To use this model"]}, {"sethuiyer/Medichat-Llama3-8B": ["### Medichat-Llama3-8B", "### Usage:", "## Quants", "## Ollama"]}, {"Locutusque/Hyperion-3.0-Mistral-7B-alpha": ["## Model Details", "## Model Description", "## Intended Use", "## Training Data", "## Quants", "## Evaluation Results", "## How to Use", "## Known Limitations", "## Licensing Information"]}, {"Locutusque/Hyperion-3.0-Yi-34B": ["## Model Details", "## Model Description", "## Intended Use", "## Training Data", "## Quants", "## Evaluation Results", "## How to Use", "## Known Limitations", "## Licensing Information"]}, {"mradermacher/JSL-MedLlama-3-8B-v9-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"hamxea/Llama-2-13b-chat-hf-activity-fine-tuned-v4": ["## Model details", "## Intended use", "## Factors", "## Metrics", "## Evaluation datasets", "## Training dataset", "## Quantitative analysis", "## Ethical considerations"]}, {"bvanaken/CORe-clinical-outcome-biobert-v1": ["## Model description", "#### How to use CORe", "### Pre-Training Data", "### More Information", "### Cite"]}, {"hamxea/Llama-2-7b-chat-hf-activity-fine-tuned-v3": ["## Model details", "## Intended use", "## Factors", "## Metrics", "## Evaluation datasets", "## Training dataset", "## Quantitative analysis", "## Ethical considerations"]}, {"hamxea/StableBeluga-7B-activity-fine-tuned-v2": ["## Model Description", "## Usage", "### System:", "### User:", "### Assistant:", "## Other Beluga Models", "## Model Details", "### Training Dataset", "### Training Procedure", "## Ethical Considerations and Limitations", "## How to cite", "## Citations"]}, {"hamxea/Mistral-7B-v0.1-activity-fine-tuned-v3": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact", "### Framework versions"]}, {"hamxea/Mistral-7B-v0.1-activity-fine-tuned-v2": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact", "### Framework versions"]}, {"sethuiyer/MedleyMD": ["## Benchmark", "## \ud83e\udde9 Configuration", "## GGUF", "## Ollama", "## Prompt format:", "## \ud83d\udcbb Usage"]}, {"sethuiyer/Dr_Samantha-7b": ["## Overview", "## Prompt Template", "### Instruction:", "### Response:", "## \u26a1 Quantized models", "## OpenLLM Leaderboard Performance", "## Subject-wise Accuracy", "## Evaluation by GPT-4 across 25 random prompts from ChatDoctor-200k Dataset", "### Overall Rating: 83.5/100", "#### Pros:", "#### Cons:"]}, {"kimou605/BioTATA-7B": ["## Model Details", "### Model Description", "### Model Sources ", "## How to Get Started with the Model", "## Bias, Risks, and Limitations", "### Recommendations", "## Training Details", "### Training Data", "### Training Procedure", "#### Training Hyperparameters", "#### Speeds, Sizes, Times ", "## Environmental Impact", "## Model Card Contact"]}, {"johnsnowlabs/JSL-MedLlama-3-8B-v1.0": ["## \ud83d\udcbb Usage", "## \ud83c\udfc6 Evaluation"]}, {"medalpaca/medalpaca-13b": ["## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Model Usage", "## Limitations"]}, {"johnsnowlabs/JSL-MedPhi2-2.7B": ["## \ud83d\udcbb Usage", "## \ud83c\udfc6 Evaluation"]}, {"pszemraj/long-t5-tglobal-base-sci-simplify": ["## Model description", "## Usage ", "## Intended uses & limitations", "## Training procedure", "### Eval results", "### Training hyperparameters", "### Training results"]}, {"mradermacher/Llama3-Aloe-8B-Alpha-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"mradermacher/Medichat-V2-Llama3-8B-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"wisdomik/QuiltNet-B-32": ["## QuiltNet-B-32 Description", "## Direct Use", "## Downstream Use", "### Intended Use", "#### Primary intended uses", "### Out-of-Scope Use Cases", "## Training Data"]}, {"alabnii/jmedroberta-base-sentencepiece-vocab50000": ["## Model description", "#### Reference", "## Datasets used for pre-training", "## How to use", "## Tokenization", "## Vocabulary", "## Training procedure", "## Note: Why do we call our model RoBERTa, not BERT?", "## Acknowledgements"]}, {"Locutusque/Hercules-4.0-Mistral-v0.2-7B": ["## Model Description", "## Quants", "## Intended Uses & Potential Bias", "## Limitations and Risks", "## Training Procedure", "## Evaluation"]}, {"wisdomik/Quilt-Llava-v1.5-7b": ["## Model details", "## License", "## Intended use", "## Training dataset", "## Evaluation dataset"]}, {"FremyCompany/BioLORD-2023-C": ["## Sibling models", "## Training strategy", "### Summary of the 3 phases", "### Contrastive phase: details", "### Self-distallation phase: details", "## Citation", "## Usage (Sentence-Transformers)", "## Usage (HuggingFace Transformers)", "## License"]}, {"HiTZ/Medical-mT5-large": ["## How to Get Started with the Model", "## Training Data", "## Evaluation", "### Medical mT5 for Sequence Labelling", "### Single-task supervised F1 scores for Sequence Labelling", "### Multi-task supervised F1 scores for Sequence Labelling", "### Zero-shot F1 scores for Argument Mining. Models have been trained in English and evaluated in Spanish, French and Italian.", "## Ethical Statement", "## Citation"]}, {"LeroyDyer/Mixtral_AI_Cyber_Boss": []}, {"TheBloke/medalpaca-13B-GPTQ": ["## Description", "## Repositories available", "## Prompt template: Alpaca", "### Instruction:", "### Response:", "## Provided files and GPTQ parameters", "## How to download from branches", "## How to easily download and use this model in [text-generation-webui](https://github.com/oobabooga/text-generation-webui).", "## How to use this GPTQ model from Python code", "### Install the necessary packages", "### For CodeLlama models only: you must use Transformers 4.33.0 or later.", "### You can then use the following code", "### Instruction:", "### Response:", "## Compatibility", "## Discord", "## Thanks, and how to contribute", "## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Model Usage", "## Limitations"]}, {"Locutusque/SlimHercules-4.0-Mistral-7B-v0.2": ["## Model Description", "## Quants", "### EXL2 [@bartowski](https://huggingface.co/bartowski/)", "### GGUF [@bartowski](https://huggingface.co/bartowski/)", "### AWQ [@solidrust](https://huggingface.co/solidrust)", "## Intended Uses & Potential Bias", "## Limitations and Risks", "## Training Procedure", "## Evaluation"]}, {"LeroyDyer/Mixtral_AI_Cyber_MegaMind_3.0_1x4": ["### Models Merged", "## CyberSeries", "## Objective: Creating a Blend of Experts!", "## Core Model: Mistral-7B-Instruct-v0.2", "## SUB MODELS - POPULAR ONES ", "## Extended capabilities:", "### Configuration"]}, {"LeroyDyer/Mixtral_AI_CyberBrain_Coder_1x2": ["### The CyberCoder :", "## Cyber ORCA :", "###  CyberDolphin "]}, {"medicalai/ClinicalGPT-base-zh": ["## Model Fine-tuning", "## How to use the model", "## Limitations"]}, {"ShieldX/manovyadh-1.1B-v1-chat": ["## Model Description", "## Direct Use", "## Downstream Use", "## Out-of-Scope Use", "## Model Architecture and Objective", "### Hardware", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"Dr-BERT/DrBERT-7GB": ["## 3.1 Install dependencies", "## 3.2 Download NACHOS Dataset text file", "## 3.3 Build your own tokenizer from scratch based on NACHOS", "## 3.4 Preprocessing and tokenization of the dataset", "## 3.5 Model training", "### 3.5.1 Pre-training from scratch", "### 3.5.2 continue pre-training"]}, {"internistai/base-7b-v0.2": ["## Model Details", "### Model Sources", "## Uses", "### Format", "### Out-of-Scope Use", "## Professional Evaluation", "## Training Details", "### Training Data", "### Training Procedure ", "#### Training Hyperparameters", "## Evaluation", "### Testing Data & Metrics", "#### Testing Data", "#### Metrics", "### Results", "## Citation"]}, {"mradermacher/OpenBioLLM-Llama3-70B-i1-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"johnsnowlabs/JSL-MedMNX-7B": ["## \ud83d\udcbb Usage", "## \ud83c\udfc6 Evaluation"]}, {"johnsnowlabs/JSL-MedMNX-7B-SFT": ["## \ud83d\udcbb Usage", "## \ud83c\udfc6 Evaluation"]}, {"skumar9/Llama-medx_v2": []}, {"QuantFactory/Medichat-Llama3-8B-GGUF": ["### Usage:", "## Ollama"]}, {"PharMolix/BioMedGPT-LM-7B": ["### Training Details", "### Model Developers", "### How to Use", "### Technical Report", "### GitHub", "### Limitations", "### Licenses"]}, {"Henrychur/MMedLM2": ["## Introduction", "## News", "## Evaluation on MMedBench", "## Contact", "## Citation"]}, {"MaziyarPanahi/BioMistral-7B-GGUF": ["## Description", "## How to use", "### About GGUF", "### Explanation of quantisation methods", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using llama-cpp-python", "#### First install the package", "#### Simple llama-cpp-python example code", "## How to use with LangChain"]}, {"BioMistral/BioMistral-7B-SLERP": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"TheBloke/medicine-LLM-13B-GGUF": ["## Description", "### About GGUF", "## Repositories available", "## Prompt template: Llama-2-Chat", "## Compatibility", "## Explanation of quantisation methods", "## Provided files", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using llama-cpp-python", "#### First install the package", "#### Simple llama-cpp-python example code", "## How to use with LangChain", "## Discord", "## Thanks, and how to contribute", "### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"microsoft/BioGPT-Large": ["## BioGPT", "## Citation"]}, {"collaiborateorg/Collaiborator-MEDLLM-Llama-3-8B-v1": ["## Model details", "## Model description", "## Quick Demo", "## Intended uses & limitations", "## Limitations and Ethical Considerations", "## Training and evaluation", "## How to use", "### Contact Information", "### Training hyperparameters", "### Framework versions", "### Citation"]}, {"ProdicusII/ZeroShotBioNER": ["## Model description", "## Example of usage", "## Example of fine-tuning with few-shot learning", "## Available classes", "## Code availibility", "## Citation"]}, {"unikei/bert-base-proteins": ["## Intended uses", "## How to use in your code"]}, {"nijatzeynalov/azerbaijani-medical-question-classification": []}, {"DATEXIS/CORe-clinical-diagnosis-prediction": ["## Model description", "#### Model Predictions", "#### How to use CORe Diagnosis Prediction", "### More Information", "### Cite"]}, {"urchade/gliner_large_bio-v0.1": []}, {"TheBloke/medicine-LLM-GGUF": ["## Description", "### About GGUF", "## Repositories available", "## Prompt template: AdaptLLM", "### User Input:", "### Assistant Output:", "## Compatibility", "## Explanation of quantisation methods", "## Provided files", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using llama-cpp-python", "#### First install the package", "#### Simple llama-cpp-python example code", "## How to use with LangChain", "## Discord", "## Thanks, and how to contribute", "### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"AdaptLLM/medicine-LLM": ["### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"TheBloke/medalpaca-13B-GGUF": ["## Description", "### About GGUF", "## Repositories available", "## Prompt template: Alpaca", "### Instruction:", "### Response:", "## Compatibility", "## Explanation of quantisation methods", "## Provided files", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using ctransformers", "#### First install the package", "#### Simple ctransformers example code", "## How to use with LangChain", "## Discord", "## Thanks, and how to contribute", "## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Model Usage", "## Limitations"]}, {"mradermacher/Palmyra-Med-70B-i1-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"TheBloke/medicine-chat-GGUF": ["## Description", "### About GGUF", "## Repositories available", "## Prompt template: Llama-2-Chat", "## Compatibility", "## Explanation of quantisation methods", "## Provided files", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using llama-cpp-python", "#### First install the package", "#### Simple llama-cpp-python example code", "## How to use with LangChain", "## Discord", "## Thanks, and how to contribute", "### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"AdaptLLM/medicine-LLM-13B": ["### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"Imran1/MedChat3.5": ["## Model Details", "### Model Description", "## Dataset Information", "### Dataset Name: Llama2-MedTuned-Instructions", "#### Dataset Description", "#### Source Datasets and Composition", "#### Prompting Strategy", "#### Usage and Application", "## Inference Instructions"]}, {"BioMistral/BioMistral-7B-GGUF": []}, {"FremyCompany/BioLORD-2023-M": ["## Sibling models", "## Training strategy", "### Summary of the 3 phases", "### Contrastive phase: details", "### Self-distallation phase: details", "## Citation", "## Usage (Sentence-Transformers)", "## Usage (HuggingFace Transformers)", "## License"]}, {"bartowski/JSL-MedLlama-3-8B-v2.0-GGUF": ["## Llamacpp imatrix Quantizations of JSL-MedLlama-3-8B-v2.0", "## Prompt format", "## Download a file (not the whole branch) from below:", "## Downloading using huggingface-cli", "## Which file should I choose?"]}, {"TheBloke/meditron-70B-GGUF": ["## Description", "### About GGUF", "## Repositories available", "## Prompt template: ChatML", "## Compatibility", "## Explanation of quantisation methods", "## Provided files", "### Q6_K and Q8_0 files are split and require joining", "### q6_K ", "### q8_0", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using llama-cpp-python", "#### First install the package", "#### Simple llama-cpp-python example code", "## How to use with LangChain", "## Discord", "## Thanks, and how to contribute", "## Model Details", "### Model Sources", "## Uses", "### Direct Use", "### Downstream Use", "### Out-of-Scope Use", "## Truthfulness, Helpfulness, Risk, and Bias", "### Recommendations", "## Training Details", "### Training Data", "#### Data Preprocessing", "### Training Procedure", "#### Training Hyperparameters", "#### Speeds, Sizes, Times", "## Evaluation", "### Testing Data & Metrics", "#### Testing Data", "#### Metrics", "### Results", "## Environmental Impact", "## Citation"]}, {"TheBloke/Asclepius-13B-GGUF": ["## Description", "### About GGUF", "## Repositories available", "## Prompt template: Asclepius", "## Compatibility", "## Explanation of quantisation methods", "## Provided files", "## How to run in `llama.cpp`", "## How to run in `text-generation-webui`", "## Discord", "## Thanks, and how to contribute.", "## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Citation [optional]"]}, {"mradermacher/Hyperion-3.0-Yi-34B-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"mradermacher/Hyperion-3.0-Yi-34B-i1-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"mradermacher/OpenBioLLM-Llama3-70B-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"ugaray96/biobert_ncbi_disease_ner": []}, {"liyuesen/druggpt": ["## \ud83d\udea9 Introduction", "## \ud83d\udce5 Deployment", "## \ud83d\udddd How to use", "## \ud83d\udd2c Example usage ", "## \ud83d\udcdd How to reference this work", "## \u2696 License"]}, {"knowledgator/IUPAC2SMILES-canonical-small": ["## Model Details", "### Model Description", "### Model Sources", "## Quickstart", "### IUPAC to SMILES", "#### To perform simple translation, follow the example:", "#### Processing in batches:", "## Bias, Risks, and Limitations", "### Training Procedure", "## Evaluation", "## Citation", "## Model Card Authors", "## Model Card Contact"]}, {"Narrativaai/BioGPT-Large-finetuned-chatdoctor": ["## Intended Use", "## Limitations", "## Model", "## Dataset", "## Usage", "### Instruction:", "### Input:", "### Response:", "## Citation"]}, {"liminghong/DNABERT-2-117M": []}, {"winninghealth/WiNGPT2-Llama-3-8B-Chat-GGUF": ["## WiNGPT2", "## \u66f4\u65b0\u65e5\u5fd7", "## \u5982\u4f55\u4f7f\u7528", "### \u63a8\u7406", "## \u8f93\u51fa\u7ed3\u679c\uff1a\u4f60\u597d\uff01\u4eca\u5929\u6211\u80fd\u4e3a\u4f60\u505a\u4e9b\u4ec0\u4e48\uff1f<|end_of_text|>", "### \u63d0\u793a", "## \u6a21\u578b\u5361", "####  \u8bad\u7ec3\u914d\u7f6e\u4e0e\u53c2\u6570", "#### \u8bad\u7ec3\u6570\u636e", "## \u4e2d\u6587\u533b\u7597\u8bc4\u6d4b - WiNEval", "### \u4f01\u4e1a\u670d\u52a1", "## \u5c40\u9650\u6027\u4e0e\u514d\u8d23\u58f0\u660e", "## \u8bb8\u53ef\u8bc1", "## \u8054\u7cfb\u6211\u4eec"]}, {"SAVSNET/PetBERT": ["## Paper Abstract", "### Model Sources", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Dataset availability statement:", "### Training Procedure ", "## Environmental Impact", "## Citation", "## Acknowledgements"]}, {"mradermacher/Hercules-4.0-Yi-34B-i1-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"BioMistral/BioMistral-7B-DARE-GGUF": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"LiteLLMs/Llama3-OpenBioLLM-70B-GGUF": ["## Description", "### About GGUF", "## Explanation of quantisation methods", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using llama-cpp-python", "#### First install the package", "#### Simple llama-cpp-python example code", "## How to use with LangChain", "## Description", "### About GGUF", "## Explanation of quantisation methods", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using llama-cpp-python", "#### First install the package", "#### Simple llama-cpp-python example code", "## How to use with LangChain", "### Use with transformers", "## **Training procedure**", "### **Training hyperparameters**", "### **Peft hyperparameters**", "### **Training results**", "### **Framework versions**", "## Detailed Medical Subjectwise accuracy"]}, {"knowledgator/IUPAC2SMILES-canonical-base": ["## Model Details", "### Model Description", "### Model Sources", "## Quickstart", "### IUPAC to SMILES", "#### To perform simple translation, follow the example:", "#### Processing in batches:", "## Bias, Risks, and Limitations", "### Training Procedure", "## Evaluation", "## Citation", "## Model Card Authors", "## Model Card Contact"]}, {"mradermacher/medalpaca-13b-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"mradermacher/Mixtral_AI_Cyber_4.0-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"QuantFactory/internistai-base-7b-v0.2-GGUF": ["## Model Details", "### Model Sources", "## Uses", "### Format", "### Out-of-Scope Use", "## Professional Evaluation", "## Training Details", "### Training Data", "### Training Procedure ", "#### Training Hyperparameters", "## Evaluation", "### Testing Data & Metrics", "#### Testing Data", "#### Metrics", "### Results"]}, {"mradermacher/JSL-MedMX-7X-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"MaziyarPanahi/medicine-chat-GGUF": ["## Description", "## How to use", "### About GGUF", "### Explanation of quantisation methods", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using llama-cpp-python", "#### First install the package", "#### Simple llama-cpp-python example code", "## How to use with LangChain"]}, {"NitzanBar/umls-spanbert": []}, {"Zabihin/Symptom_to_Diagnosis": ["## Model description", "## Dataset Information", "### Framework versions"]}, {"wisdomik/QuiltNet-B-16": ["## QuiltNet-B-16 Description", "## Direct Use", "## Downstream Use", "### Intended Use", "#### Primary intended uses", "### Out-of-Scope Use Cases", "## Training Data"]}, {"mradermacher/Mixtral_AI_Cyber_MegaMind_3_0-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"mradermacher/JSL-MedLlama-3-70B-v1.0-i1-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"alchemab/antiberta2-cssp": ["## Example usage"]}, {"mradermacher/meerkat-7b-v1.0-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"bartowski/Hyperion-3.0-Mistral-7B-alpha-GGUF": ["## Llamacpp Quantizations of Hyperion-3.0-Mistral-7B-alpha"]}, {"unikei/bert-base-smiles": ["## Intended uses", "## How to use in your code"]}, {"mradermacher/JSL-MedLlama-3-8B-v1.0-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"mradermacher/Palmyra-Med-70B-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"BioMistral/BioMistral-7B-AWQ-QGS128-W4-GEMM": []}, {"afrideva/tiny-llama-1.1b-chat-medical-GGUF": ["## Original Model Card:"]}, {"bartowski/Hercules-4.0-Mistral-v0.2-7B-GGUF": ["## Llamacpp Quantizations of Hercules-4.0-Mistral-v0.2-7B"]}, {"mradermacher/Qwen1.5-7B-MeChat-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"DunnBC22/opus-mt-de-en-OPUS_Medical_German_to_English": ["### Model description", "### Intended uses & limitations", "### Training and evaluation data", "#### Histogram of German Input Word Counts", "#### Histogram of English Input Word Counts", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"Isaak-Carter/Hyperion-2.0-Mistral-7B-GGUF": ["## Model Details", "## Model Description", "## Intended Use", "## Training Data", "## Evaluation Results", "## How to Use", "## Known Limitations", "## Licensing Information"]}, {"mradermacher/JSL-MedLlama-3-70B-v1.0-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"jianghc/medical_chatbot": ["## Quickstart", "## Example Outputs"]}, {"bartowski/SlimHercules-4.0-Mistral-7B-v0.2-GGUF": ["## Llamacpp Quantizations of SlimHercules-4.0-Mistral-7B-v0.2", "## Prompt format", "## Download a file (not the whole branch) from below:", "## Which file should I choose?"]}, {"mradermacher/Mixtral_AI_Cyber_2.0-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"dmis-lab/meerkat-7b-v1.0": ["## Quick Start", "## Prompt Details", "### USMLE or Clinical Cases", "### Multiple-choice Exams", "### Other Use Cases", "## Model Architecture", "## Training Data", "## Contact"]}, {"mradermacher/Mr-Grammatology-clinical-problems-Mistral-7B-0.5-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"bartowski/Hercules-3.1-Mistral-7B-GGUF": ["## Llamacpp Quantizations of Hercules-3.1-Mistral-7B"]}, {"bartowski/Hercules-4.0-Yi-34B-GGUF": ["## Llamacpp Quantizations of Hercules-4.0-Yi-34B"]}, {"mradermacher/Mixtral_AI_Medic-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"bartowski/NeuralHyperion-2.0-Mistral-7B-GGUF": ["## Llamacpp Quantizations of NeuralHyperion-2.0-Mistral-7B"]}, {"bartowski/Hyperion-2.0-Mistral-7B-GGUF": ["## Llamacpp Quantizations of Hyperion-2.0-Mistral-7B"]}, {"bartowski/Hyperion-3.0-Yi-34B-GGUF": ["## Llamacpp Quantizations of Hyperion-3.0-Yi-34B"]}, {"iandennismiller/LLama-2-MedText-13b-GGUF": ["## Usage", "## Model card from truehealth/Llama-2-MedText-Delta-Preview", "## Setup Notes", "### Download torch model", "### Quantize torch model with llama.cpp", "### Distributing model through huggingface"]}, {"mradermacher/JSL-MedLX-70B-v3-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"sbawa/elysa-gguf": []}, {"mradermacher/JSL-MedMNX-7B-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"Locutusque/Llama-3-NeuralHercules-5.0-8B": ["## Model Details", "## Model Description", "## Intended Use", "## Training Data", "## Evaluation Results", "## How to Use", "## Known Limitations"]}, {"MaziyarPanahi/BioMedGPT-LM-7B-GGUF": ["## Description", "## How to use", "### About GGUF", "### Explanation of quantisation methods", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using llama-cpp-python", "#### First install the package", "#### Simple llama-cpp-python example code", "## How to use with LangChain"]}, {"bartowski/JSL-MedLlama-3-8B-v1.0-GGUF": ["## Llamacpp imatrix Quantizations of JSL-MedLlama-3-8B-v1.0", "## Prompt format", "## Download a file (not the whole branch) from below:", "## Downloading using huggingface-cli", "## Which file should I choose?"]}, {"ayoubkirouane/Breast-Cancer_SAM_v1": ["## Description :", "## Base Model:", "## Get Started with the Model"]}, {"MaziyarPanahi/BioMistral-7B-SLERP-GGUF": ["## Description", "## How to use", "### About GGUF", "### Explanation of quantisation methods", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using llama-cpp-python", "#### First install the package", "#### Simple llama-cpp-python example code", "## How to use with LangChain"]}, {"MaziyarPanahi/BioMistral-7B-TIES-GGUF": ["## Description", "## How to use", "### About GGUF", "### Explanation of quantisation methods", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using llama-cpp-python", "#### First install the package", "#### Simple llama-cpp-python example code", "## How to use with LangChain"]}, {"mradermacher/Hercules-4.0-Yi-34B-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"MaziyarPanahi/BioMistral-7B-DARE-GGUF": ["## Description", "## How to use", "### About GGUF", "### Explanation of quantisation methods", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using llama-cpp-python", "#### First install the package", "#### Simple llama-cpp-python example code", "## How to use with LangChain"]}, {"mradermacher/JSL-MedLX-70B-v2-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"mradermacher/JSL-MedLX-70B-v2-i1-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"UMCU/MedRoBERTa.nl_NegationDetection": ["## Description", "## Minimal example", "## Intended use", "## Data", "## Authors", "## Contact", "## Usage", "## References"]}, {"portugueseNLP/medialbertina_pt-pt_900m": ["## Data", "## How to use", "## Citation"]}, {"DATEXIS/CORe-clinical-mortality-prediction": ["## Model description", "#### How to use CORe Mortality Risk Prediction", "### More Information", "### Cite"]}, {"mradermacher/Mixtral_AI_Cyber_5.0-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"Medilora/Medilora-Mistral-7B": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"sethuiyer/MedleyMD-GGUF": []}, {"shanover/symps_disease_bert_v3_c41": ["## List of Encoded Numbers and Corresponding Disease Names"]}, {"MaziyarPanahi/Bioxtral-4x7B-v0.1-GGUF": ["## Description", "## How to use", "### About GGUF", "### Explanation of quantisation methods", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model in Python code, using llama-cpp-python", "#### First install the package", "#### Simple llama-cpp-python example code", "## How to use with LangChain"]}, {"UMCU/MedRoBERTa.nl_Experiencer": ["## Description", "## Minimal example", "## Intended use", "## Data", "## Authors", "## Contact", "## Usage", "## References"]}, {"johnsnowlabs/JSL-MedLlama-3-8B-v9": ["## \ud83d\udcbb Usage"]}, {"jihadzakki/blip1-medvqa": []}, {"abazoge/DrLongformer": ["### Model pretraining", "### Model Usage", "### Citation"]}, {"Flmc/DISC-MedLLM": ["## Overview", "## Dataset", "## Deploy", "### Using through hugging face transformers", "## Training", "## Delcaration", "## Licenses", "## Citation"]}, {"microsoft/llava-med-7b-delta": ["## Model Description ", "### Model Uses ", "#### Intended Use ", "#### Primary Intended Use ", "#### Out-of-Scope Use ", "### Data ", "### Limitations ", "## Install", "## Serving", "## Evaluation", "### Medical Visual Chat (GPT-assisted Evaluation)", "### Medical VQA", "#### - Prepare Data", "#### - Fine-tuning", "#### - Evaluation", "## Acknowledgement", "## Related Projects"]}, {"Idan0405/ClipMD": ["## Model Details", "### Model Description", "## Use with Transformers"]}, {"jaandoui/DNABERT2-AttentionExtracted": []}, {"LoneStriker/BioMistral-7B-SLERP-GPTQ": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"Posos/ClinicalNER": ["## Model Description", "## Evaluation Metrics on [MedNERF dataset](https://huggingface.co/datasets/Posos/MedNERF)", "## Usage", "## Citation information"]}, {"Locutusque/gpt2-large-medical": []}, {"Henrychur/MMed-Llama-3-8B": ["## Introduction", "## News", "## Evaluation on MMedBench", "## Contact", "## Citation"]}, {"ryanyip7777/pmc_vit_l_14": ["### Model Description", "###  Training"]}, {"Kquant03/BioMistral-7B-TIES-GGUF": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"mradermacher/JSL-MedMNX-7B-v4.0-GGUF": ["## About", "## Usage", "## Provided Quants", "## FAQ / Model Request", "## Thanks"]}, {"SumayyaAli/tiny-llama-1.1b-chat-medical": []}, {"hwonheo/Llama-3-medquad-8B-GGUF": ["## License", "## Llama 3 Intended Use"]}, {"iioSnail/bert-base-chinese-medical-ner": []}, {"medkit/DrBERT-CASM2": ["## Model description", "## Limitations and bias", "## Install medkit", "## Using the model", "## Training procedure", "## How to evaluate using medkit"]}, {"BioMistral/BioMistral-7B-AWQ-QGS128-W4-GEMV": []}, {"sethuiyer/Dr.Samantha-8B": ["### Models Merged", "### Configuration", "## Usage:", "## Quants"]}, {"havocy28/VetBERT": ["## Pretraining Data", "## Pretraining Hyperparameters", "## VetBERT Finetuning", "## How to use the model", "## Citation"]}, {"kingabzpro/llama-3-8b-chat-doctor": ["## llama-3-8b-chat-doctor", "## Files"]}, {"jihadzakki/idefics2-8b-vqarad-delta": []}, {"nlpie/Llama2-MedTuned-7b": []}, {"summit4you/Llama3-8B-COIG-CQIA-GGUF": []}, {"portugueseNLP/medialbertina_pt-pt_1.5b_NER": ["## Data", "## How to use", "## Citation"]}, {"hansmueller464/Llama3-Aloe-8B-Alpha-Q6_K-GGUF": ["## Use with llama.cpp"]}, {"nlpie/Llama2-MedTuned-13b": []}, {"LeroyDyer/Mixtral_AI_Vision-Instruct_X_7b": ["## LeroyDyer/Mixtral_AI_Vision-Instruct_X", "## Vision/multimodal capabilities:", "## Extended capabilities:", "## using transformers", "## Using pipeline", "## Mistral ChatTemplating"]}, {"Neurai/NeuraMedicalGemma7b": ["## Model Description", "### Model Sources", "## Uses", "### Instruction:{}", "### Input:{}", "### Response:{}\"\"\"", "## More Information", "## Model Card Authors", "## Model Card Contact"]}, {"DoctorDiffusion/doctor-diffusion-s-xray-xl-lora-0": ["## Model description", "## Trigger words", "## Download model", "## Use it with the [\ud83e\udde8 diffusers library](https://github.com/huggingface/diffusers)"]}, {"egeozsoy/EndoViT": ["## \u270f\ufe0f Citation"]}, {"DoctorDiffusion/doctor-diffusion-s-xray-xl-lora": ["## Model description", "## Trigger words", "## Download model", "## Use it with the [\ud83e\udde8 diffusers library](https://github.com/huggingface/diffusers)"]}, {"LoneStriker/BioMistral-7B-GGUF": []}, {"portugueseNLP/medialbertina_pt-pt_900m_NER": ["## Data", "## How to use", "## Citation"]}, {"Henrychur/MMedLM": ["## Introduction", "## News", "## Evaluation on MMedBench", "## Contact", "## Citation"]}, {"raylim/QuiltNet-B-16-PMB": ["## QuiltNet-B-16-PMB Description", "## Direct Use", "## Downstream Use", "### Intended Use", "#### Primary intended uses", "### Out-of-Scope Use Cases", "## Training Data"]}, {"TheBloke/meditron-70B-AWQ": ["## Description", "### About AWQ", "## Repositories available", "## Prompt template: ChatML", "## Provided files, and AWQ parameters", "## How to easily download and use this model in [text-generation-webui](https://github.com/oobabooga/text-generation-webui)", "## Multi-user inference server: vLLM", "## Multi-user inference server: Hugging Face Text Generation Inference (TGI)", "## Inference from Python code using Transformers", "### Install the necessary packages", "### Transformers example code (requires Transformers 4.35.0 and later)", "## Compatibility", "## Discord", "## Thanks, and how to contribute", "## Model Details", "### Model Sources", "## Uses", "### Direct Use", "### Downstream Use", "### Out-of-Scope Use", "## Truthfulness, Helpfulness, Risk, and Bias", "### Recommendations", "## Training Details", "### Training Data", "#### Data Preprocessing", "### Training Procedure ", "#### Training Hyperparameters", "#### Speeds, Sizes, Times", "## Evaluation", "### Testing Data & Metrics", "#### Testing Data", "#### Metrics", "### Results", "## Environmental Impact", "## Citation"]}, {"janhq/medicine-llm-GGUF": []}, {"MAli-Farooq/Derm-T2IM": []}, {"naotous/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224_original": ["## Citation", "## Model Use", "### How to use", "### Intended Use", "#### Primary Intended Use", "#### Out-of-Scope Use", "## Data", "## Limitations", "## Further information"]}, {"R-J/StainFuser": ["### Organisation", "### Citation"]}, {"johnsnowlabs/JSL-MedMX-7X": ["## \ud83d\udcbb Usage", "## \ud83c\udfc6 Evaluation"]}, {"TheRightHomeCareTeam/Sulis-Dr-Llama-8b-Medical-Chat": []}, {"LoneStriker/BioMistral-7B-SLERP-GGUF": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"suinleelab/monet": ["## Description", "## Citation", "## Model Details", "### Model Type", "## Model Use", "### Intended Use", "#### Primary intended uses", "### Out-of-Scope Use Cases"]}, {"starmpcc/Asclepius-7B": ["## UPDATE", "### 2024.01.10", "## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Training Hyperparameters", "#### Speeds, Sizes, Times", "## Citation"]}, {"JL42/medllama3-v20-GGUF": ["## Model Description", "## Training Hyperparameters"]}, {"Dr-BERT/DrBERT-4GB": ["## 3.1 Install dependencies", "## 3.2 Download NACHOS Dataset text file", "## 3.3 Build your own tokenizer from scratch based on NACHOS", "## 3.4 Preprocessing and tokenization of the dataset", "## 3.5 Model training", "### 3.5.1 Pre-training from scratch", "### 3.5.2 continue pre-training"]}, {"JL42/NewMes-v6-GGUF": ["## Model Description"]}, {"Dr-BERT/DrBERT-4GB-CP-PubMedBERT": ["## 3.1 Install dependencies", "## 3.2 Download NACHOS Dataset text file", "## 3.3 Build your own tokenizer from scratch based on NACHOS", "## 3.4 Preprocessing and tokenization of the dataset", "## 3.5 Model training", "### 3.5.1 Pre-training from scratch", "### 3.5.2 continue pre-training"]}]