[{"nold/BioMistral-7B-GGUF": []}, {"HiTZ/Medical-mT5-large-multitask": []}, {"klyang/MentaLLaMA-chat-13B": ["## Other Models in MentaLLaMA", "## Usage", "## License", "## Citation"]}, {"TheBloke/medicine-LLM-GPTQ": ["## Repositories available", "## Prompt template: AdaptLLM", "### User Input:", "### Assistant Output:", "## Known compatible clients / servers", "## Provided files, and GPTQ parameters", "## How to download, including from branches", "### In text-generation-webui", "### From the command line", "### With `git` (**not** recommended)", "## How to easily download and use this model in [text-generation-webui](https://github.com/oobabooga/text-generation-webui)", "## Serving this model from Text Generation Inference (TGI)", "### Assistant Output:", "## Python code example: inference from this GPTQ model", "### Install the necessary packages", "### Example Python code", "### Assistant Output:", "## Compatibility", "## Discord", "## Thanks, and how to contribute", "### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"shibing624/ziya-llama-13b-medical-merged": ["## Training details", "## Usage", "## Usage (HuggingFace Transformers)", "### Inference Examples", "### \u8bad\u7ec3\u6570\u636e\u96c6", "## Citation"]}, {"klyang/MentaLLaMA-chat-7B-hf": ["## Other Models in MentaLLaMA", "## Usage", "## License", "## Citation"]}, {"portugueseNLP/medialbertina_pt-pt_1.5b": ["## Data", "## How to use", "## Citation"]}, {"MohamedAhmedAE/Llama3-8B-Medical-Finetune-Merged": ["### Evaluation results"]}, {"LoneStriker/BioMistral-7B-DARE-GGUF": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"hansmueller464/Llama3-Aloe-8B-Alpha-Q8_0-GGUF": ["## Use with llama.cpp"]}, {"LeroyDyer/Mixtral_AI_1.0_7b": ["## LeroyDyer/Mixtral_AI_1.0_7b-GGUF", "## CURRENT_ CONCEPT"]}, {"JL42/NewMes-v7-GGUF": ["## Model Description"]}, {"JL42/Llama-medx_v0-GGUF": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"squarelike/polyglot-ko-medical-chat-5.8b": []}, {"starmpcc/Asclepius-13B": ["## UPDATE", "### 2024.01.10", "## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Training Hyperparameters", "#### Speeds, Sizes, Times", "## Citation"]}, {"dattaraj/Gemma-2b-it-mental-health": ["## Model Details: Instruction tuned model foir answering questions from mental health patients.", "### Model Description: Instruction tuned model foir answering questions from mental health patients.", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"xtie/GPT2-PET-impression": []}, {"squarelike/polyglot-ko-medical-5.8b": ["## \ud559\uc2b5 \ub370\uc774\ud130", "## \ud559\uc2b5"]}, {"TheBloke/medicine-chat-GPTQ": ["## Repositories available", "## Prompt template: Llama-2-Chat", "## Known compatible clients / servers", "## Provided files, and GPTQ parameters", "## How to download, including from branches", "### In text-generation-webui", "### From the command line", "### With `git` (**not** recommended)", "## How to easily download and use this model in [text-generation-webui](https://github.com/oobabooga/text-generation-webui)", "## Serving this model from Text Generation Inference (TGI)", "## Python code example: inference from this GPTQ model", "### Install the necessary packages", "### Example Python code", "## Compatibility", "## Discord", "## Thanks, and how to contribute", "### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"LoneStriker/BioMistral-7B-TIES-GGUF": ["## Merge Details", "### Merge Method", "### Models Merged", "### Configuration"]}, {"TheBloke/PMC_LLAMA-7B-GPTQ": ["## Repositories available", "## Prompt template: Unknown", "## Provided files", "## How to download from branches", "## How to easily download and use this model in [text-generation-webui](https://github.com/oobabooga/text-generation-webui).", "## How to use this GPTQ model from Python code", "## Compatibility", "## Discord", "## Thanks, and how to contribute."]}, {"hanzohazashi1/medical_qna-gguf": []}, {"garcianacho/MedLlama-2-7B-GGUF": []}, {"DaizeDong/GraphsGPT-1W": []}, {"BioMistral/BioMistral-7B-BnB.4": []}, {"BioMistral/BioMistral-7B-BnB.8": []}, {"mrm8488/vit-base-patch16-224_finetuned-kvasirv2-colonoscopy": ["## Demo", "### Drag the following images to the widget to test the model", "## Training", "## Metrics", "## How to use"]}, {"alabnii/jmedroberta-base-sentencepiece": ["## Model description", "#### Reference", "## Datasets used for pre-training", "## How to use", "## Tokenization", "## Vocabulary", "## Training procedure", "## Note: Why do we call our model RoBERTa, not BERT?", "## Acknowledgements"]}, {"daisaku-s/medtxt_ner_roberta": ["## \u6982\u8981", "## fine-tuning\u6642\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf", "## \u4f7f\u7528\u65b9\u6cd5", "### 1", "### 2", "## \u5b9f\u9a13\u7d50\u679c (Micro-F1)", "## \u53c2\u8003\u6587\u732e"]}, {"StanfordShahLab/clmbr-t-base": ["## Model Details", "### Model Description", "### Model Sources", "## Uses", "### Direct Use", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure", "#### Preprocessing", "#### Training Hyperparameters", "## Evaluation", "## Technical Specifications", "## Vocabulary", "## Citation", "## Model Card Authors", "## Model Card Contact"]}, {"knowledgator/SMILES-DeBERTa-base": ["## Model Details", "### Model Description", "## Citation", "## Model Card Authors", "## Model Card Contact"]}, {"DavidAU/tiny-llama-1.1b-chat-medical-Q8_0-GGUF": ["## Use with llama.cpp"]}, {"ml4pubmed/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext_pub_section": ["## usage in python", "## metadata", "### training_metrics"]}, {"margotwagner/roberta-psychotherapy-eval": ["## Model Details", "### Model Description", "## Uses", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "### Metrics"]}, {"Elkhayyat17/llama2-Med-gguf": ["## Description", "### About GGUF", "## Prompt template: None", "## Compatibility", "## Explanation of quantisation methods", "## Provided files", "## How to download GGUF files", "### In `text-generation-webui`", "### On the command line, including multiple files at once", "## Example `llama.cpp` command", "## How to run in `text-generation-webui`", "## How to run from Python code", "### How to load this model from Python using ctransformers", "#### First install the package", "#### Simple example code to load one of these GGUF models", "## How to use with LangChain", "## Model Use", "## Model Details", "## Training Data", "## Evaluation Results", "## Ethical Considerations and Limitations"]}, {"ninaa510/distilbert-finetuned-medical-diagnosis": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"Druvith/mistralmed-7b-v1.5.gguf": []}, {"BiMediX/BiMediX-Eng": ["## Model Card for BiMediX-Bilingual", "### Model Details", "### Intended Use", "## Getting Started", "### Training Procedure", "### Model Performance", "### Safety and Ethical Considerations", "### Accessibility", "### Authors"]}, {"vlbthambawita/custom-deepfake": []}, {"alibidaran/llama-2-7b-virtual_doctor-gguf": ["#### This Model is optimized version of \"alibidaran/llama-2-7b-virtual_doctor\" for executing on CPU and GPU. It can be easily used on CPU and personal computers.", "## Uses "]}, {"Writer/Palmyra-Med-70B": ["## Model Details", "#### Specialized for Biomedical Applications", "### Model Description", "## Intended Use", "### Use with transformers", "## Evaluation Results", "### Performance on Biomedical Benchmarks", "### Medical Use Cases", "### Bias, Risks, and Limitations", "### Citation and Related Information"]}, {"wangrongsheng/WiNGPT2-Llama-3-8B-Chat-Q4_0-GGUF": ["## Use with llama.cpp", "### CLI:", "### Server:"]}, {"beatrice-portelli/DiLBERT": ["### Composition of the pretraining corpus", "### Main repository"]}, {"alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000": ["## Model description", "#### Reference", "## Datasets used for pre-training", "## How to use", "## Tokenization", "## Vocabulary", "## Training procedure", "## Note: Why do we call our model RoBERTa, not BERT?", "## Acknowledgements"]}, {"Harshit0722/dolly-fine-tuned-on-med-data": []}, {"nuvocare/NuvoChat": ["## Model Details", "### Model Description", "## Uses", "### Direct Use", "### Downstream Use [optional]", "## Bias, Risks, and Limitations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure"]}, {"LeroyDyer/Mixtral_AI_LCARS_WORLD_ARCHIVE": ["### RECALL: "]}, {"qiuhuachuan/PsyChat": ["## Quick Start"]}, {"chtseng/TAIDE-Medicine-QA-TW-Q6": []}, {"KindLab/roberta-deid": []}, {"LeroyDyer/Mixtral_AI_CyberBrain_Coder-Q3_K_M-GGUF": ["## Use with llama.cpp"]}, {"Hazy2028/pytorch_model-00001-of-00003.bin": ["## Introduction", "## News", "## Evaluation on MMedBench", "## Contact", "## Citation"]}, {"winninghealth/WiNGPT2-Llama-3-8B-Chat-AWQ": ["## WiNGPT2", "## \u66f4\u65b0\u65e5\u5fd7", "## \u5982\u4f55\u4f7f\u7528", "### \u63a8\u7406", "## \u8f93\u51fa\u7ed3\u679c\uff1a\u4f60\u597d\uff01\u4eca\u5929\u6211\u80fd\u4e3a\u4f60\u505a\u4e9b\u4ec0\u4e48\uff1f<|end_of_text|>", "### \u63d0\u793a", "## \u6a21\u578b\u5361", "####  \u8bad\u7ec3\u914d\u7f6e\u4e0e\u53c2\u6570", "#### \u8bad\u7ec3\u6570\u636e", "## \u4e2d\u6587\u533b\u7597\u8bc4\u6d4b - WiNEval", "### \u4f01\u4e1a\u670d\u52a1", "## \u5c40\u9650\u6027\u4e0e\u514d\u8d23\u58f0\u660e", "## \u8bb8\u53ef\u8bc1", "## \u8054\u7cfb\u6211\u4eec"]}, {"Mbilal755/Radiology_Bart": ["## Model Highlights", "### Parent Model ", "## Model Architecture", "## Data", "## Training", "## Performance", "## Usage", "## Limitations", "## Check Demo", "## Model Card Contact"]}, {"monet-joe/human-detector": ["## Human Detector", "## Maintenance", "## Mirror", "## Reference"]}, {"xtie/PEGASUS-PET-impression": ["## \ud83d\udcd1 Model Description", "## \ud83d\udcd1 Abstract", "## \ud83d\ude80 Usage", "### \ud83d\udcca Performance Metrics", "### \ud83d\udca1 Highlights", "### \ud83d\udda5\ufe0f Hardware", "## \ud83d\udcc1 Additional Resources"]}, {"dhhd255/EfficientNet_ParkinsonsPred": ["## Overview", "## Dataset", "## Usage"]}, {"Kushtrim/bert-base-cased-biomedical-ner": ["## Model Details", "## Model Description", "## Intended Use", "## Labels", "## Usage", "## Training procedure", "### Training hyperparameters", "### Framework versions"]}, {"LeroyDyer/Mixtral_AI_CyberBrain_Coder-Q2_K-GGUF": ["## Use with llama.cpp"]}, {"PantagrueLLM/jargon-general-base": ["## Evaluation", "## Using Jargon models with HuggingFace transformers", "## Citation", "### Model Sources [optional]"]}, {"Emmytheo/DiagBERT": ["## Model description", "#### Model Predictions", "#### How to use CORe Diagnosis Prediction", "### More Information", "### Cite"]}, {"DunnBC22/yolos-tiny-Brain_Tumor_Detection": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"Prajna1999/Prajna-gpt-neo-1.3B-fitbot": []}, {"BMRetriever/BMRetriever-2B": ["## Usage", "## Citation"]}, {"TheBloke/medicine-LLM-13B-GPTQ": ["## Repositories available", "## Prompt template: Llama-2-Chat", "## Known compatible clients / servers", "## Provided files, and GPTQ parameters", "## How to download, including from branches", "### In text-generation-webui", "### From the command line", "### With `git` (**not** recommended)", "## How to easily download and use this model in [text-generation-webui](https://github.com/oobabooga/text-generation-webui)", "## Serving this model from Text Generation Inference (TGI)", "## Python code example: inference from this GPTQ model", "### Install the necessary packages", "### Example Python code", "## Compatibility", "## Discord", "## Thanks, and how to contribute", "### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"LeroyDyer/Mixtral_AI_CyberTron-Q4_K_M-GGUF": ["## Use with llama.cpp"]}, {"IndianaUniversityDatasetsModels/MIMIC-Medical-Report-Generator": ["## Inputs and Outputs", "## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "### How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"xtie/BART-PET-impression": ["## \ud83d\udcd1 Model Description", "## \ud83d\udcd1 Abstract", "## \ud83d\ude80 Usage", "### \ud83d\udcca Performance Metrics", "### \ud83d\udca1 Highlights", "### \ud83d\udda5\ufe0f Hardware", "## \ud83d\udcc1 Additional Resources"]}, {"LeroyDyer/Mixtral_Cyber_BioMedic-Q4_K_M-GGUF": ["## Use with llama.cpp"]}, {"LeroyDyer/Mixtral_AI_CyberBrain_Coder_1x2-Q4_K_M-GGUF": ["## Use with llama.cpp"]}, {"alabnii/jmedroberta-base-manbyo-wordpiece": ["## Model description", "#### Reference", "## Datasets used for pre-training", "## How to use", "## Tokenization", "## Vocabulary", "## Training procedure", "## Note: Why do we call our model RoBERTa, not BERT?", "## Acknowledgements"]}, {"TheBloke/Asclepius-13B-GPTQ": ["## Description", "## Repositories available", "## Prompt template: Asclepius", "## Provided files and GPTQ parameters", "## How to download from branches", "## How to easily download and use this model in [text-generation-webui](https://github.com/oobabooga/text-generation-webui).", "## How to use this GPTQ model from Python code", "### Install the necessary packages", "### For CodeLlama models only: you must use Transformers 4.33.0 or later.", "### You can then use the following code", "## Compatibility", "## Discord", "## Thanks, and how to contribute.", "## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Citation [optional]"]}, {"Johnyquest7/thyroid_open_llama_3b_v2b_full": []}, {"TheBloke/meditron-70B-GPTQ": ["## Repositories available", "## Prompt template: ChatML", "## Known compatible clients / servers", "## Provided files, and GPTQ parameters", "## How to download, including from branches", "### In text-generation-webui", "### From the command line", "### With `git` (**not** recommended)", "## How to easily download and use this model in [text-generation-webui](https://github.com/oobabooga/text-generation-webui)", "## Serving this model from Text Generation Inference (TGI)", "## Python code example: inference from this GPTQ model", "### Install the necessary packages", "### Example Python code", "## Compatibility", "## Discord", "## Thanks, and how to contribute", "## Model Details", "### Model Sources", "## Uses", "### Direct Use", "### Downstream Use", "### Out-of-Scope Use", "## Truthfulness, Helpfulness, Risk, and Bias", "### Recommendations", "## Training Details", "### Training Data", "#### Data Preprocessing", "### Training Procedure ", "#### Training Hyperparameters", "#### Speeds, Sizes, Times", "## Evaluation", "### Testing Data & Metrics", "#### Testing Data", "#### Metrics", "### Results", "## Environmental Impact", "## Citation"]}, {"ThisIs-Developer/Llama-2-GGML-Medical-Chatbot": ["## \ud83d\udcda Here are some of the features of the Llama-2-7B-Chat-GGML-Medical-Chatbot:", "## \ud83d\ude80 Quickstart", "## \ud83d\udcd6 ChatBot Conversession", "### \u26d3\ufe0fChainlit ver. on [#v1.0.1.dev20230913](https://github.com/ThisIs-Developer/Llama-2-GGML-Medical-Chatbot/releases/tag/v1.0.1.dev20230913)", "### \u26a1Streamlit ver. on [#v2.0.1.dev20231230](https://github.com/ThisIs-Developer/Llama-2-GGML-Medical-Chatbot/releases/tag/v2.0.1.dev20231230)", "### DEMO: \ud83d\udcfd\ufe0fConversession.vid.mp4->https://cdn-uploads.huggingface.co/production/uploads/64d8c442a4839890b2490db9/iI4t0lhjkCw3dDSvWQ4Jk.mp4"]}, {"BiMediX/BiMediX-Bi": ["## Model Card for BiMediX-Bilingual", "### Model Details", "### Intended Use", "## Getting Started", "### Training Procedure", "### Model Performance", "### Safety and Ethical Considerations", "### Accessibility", "### Authors"]}, {"miloradg/base-7b-v0.2-Q8_0-GGUF": ["## Use with llama.cpp"]}, {"alimoezzi/ReportQL-base": ["## Introduction", "## Dataset", "## Setup", "### Setting up environments and Installing dependencies", "### Installing dependencies", "### Fine-tuning", "### Testing", "### Inference", "## Fine-tuned Model", "## License", "## Citation"]}, {"hamzamalik11/Biobart_radiology_summarization": ["## Model Details", "### Model Description", "### Model Sources ", "## Uses", "### Direct Use", "### Out-of-Scope Use", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Training Hyperparameters", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Card Authors ", "## Model Card Contact"]}, {"DunnBC22/yolos-small-Liver_Disease": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"Zamoranesis/mental_bert": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"LeroyDyer/Mixtral_AI_Cyber_MegaMind-Q4_K_M-GGUF": ["## Use with llama.cpp"]}, {"LeroyDyer/Mixtral_AI_Cyber_MegaMind-Q3_K_S-GGUF": ["## Use with llama.cpp"]}, {"cccmatthew/surrey-gp30": []}, {"davanstrien/CamemBERT-MedNERF": ["## Validation Metrics", "## Usage"]}, {"kaveh/rclip": ["## Heatmap", "## Image Retrieval", "### 1-Save Image Embeddings", "### 2-Query for Images", "## Zero-Shot Image Classification", "## Metrics", "## Hyperparameters", "## Framework Versions", "## Citation"]}, {"AnsumanBhujabal/Indian-Ayurvedic-Products": ["## Example Images", "#### Aloevera", "#### Lavender", "#### Neem", "#### Tulsi", "#### Turmeric"]}, {"Mohammed-Altaf/medical_chatbot-8bit": []}, {"LeroyDyer/Mixtral_AI_Cyber_MegaMind_2_0-Q4_K_S-GGUF": ["## Use with llama.cpp"]}, {"LeroyDyer/Mixtral_AI_1x4": ["## Use with llama.cpp"]}, {"ChantalPellegrini/RaDialog-interactive-radiology-report-generation": ["## Get Started", "## \u270f\ufe0f Citation"]}, {"LeroyDyer/Mixtral_AI_MasterMind-Q4_K_M-GGUF": ["## Use with llama.cpp"]}, {"filepile/medical-llama3-8b": []}, {"mrm8488/biomedtra-small-finenuned-clinical-ner": []}, {"ml4pubmed/scibert-scivocab-uncased_pub_section": ["## usage in python", "## metadata", "### training_metrics", "### training_parameters"]}, {"KindLab/bert-deid": []}, {"ayoubkirouane/Med_English2Spanish": ["## Model Description", "## About Dataset:", "### Dataset Statistics:", "## Ethical Considerations", "## Intended Use", "## Limitations", "## Usage "]}, {"bcwarner/audit-icu-gpt2-25_3M": []}, {"LeroyDyer/Mixtral_AI_LongTalker-Q4_K_S-GGUF": ["## Use with llama.cpp"]}, {"thesven/Llama3-8B-SFT-SyntheticMedical-bnb-4bit": ["## Model Details", "### Model Description", "### Using the model with transformers"]}, {"winninghealth/WiNGPT2-7B-Chat": ["## WiNGPT2", "## \u4ecb\u7ecd", "## \u7279\u70b9", "## \u5982\u4f55\u4f7f\u7528", "### \u63a8\u7406", "## \u8f93\u51fa\u7ed3\u679c\uff1a\u4f60\u597d\uff01\u4eca\u5929\u6211\u80fd\u4e3a\u4f60\u505a\u4e9b\u4ec0\u4e48\uff1f<|endoftext|>", "### \u63d0\u793a", "### \u4f01\u4e1a\u670d\u52a1", "## \u8bad\u7ec3\u6570\u636e", "## \u6a21\u578b\u5361", "## \u8bc4\u6d4b", "## \u5c40\u9650\u6027\u4e0e\u514d\u8d23\u58f0\u660e", "## \u8bb8\u53ef\u8bc1", "## \u53c2\u8003\u8d44\u6599", "## \u8054\u7cfb\u6211\u4eec"]}, {"HumaP/vit_base_patch16_224_in21k_lung_and_colon_histopathology_pt": ["## Description  ", "## About Dataset", "## Training Results", "## How to use"]}, {"alibidaran/Gemma2_Virtual_doctor": ["## Model Details", "### Model Description", "## Uses", "## Training Parameters"]}, {"LeroyDyer/Mixtral_AI_Cyber_MegaMind_1x4_SFT-Q4_K_M-GGUF": ["## Use with llama.cpp"]}, {"LeroyDyer/Mixtral_AI_Cyber_MegaMind_1x4-Q4_K_M-GGUF": ["## Use with llama.cpp"]}, {"fakezeta/Llama3-Aloe-8B-Alpha-ov-int8": ["## Model Details", "### [](https://huggingface.co/templates/model-card-example#model-description)Model Description", "### [](https://huggingface.co/templates/model-card-example#model-sources-optional)Model Sources [optional]", "## Model Performance", "## Uses", "### Direct Use", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "#### Transformers pipeline", "#### Transformers AutoModelForCausalLM", "## Training Details", "### Training Data", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Metrics", "### Results", "#### Summary", "## Environmental Impact", "## Model Card Contact"]}, {"KindLab/distilbert-deid": []}, {"Dr-BERT/CAS-Biomedical-POS-Tagging": []}, {"Chhabi/mt5-small-finetuned-Nepali-Health-50k-2": ["### Training Procedure", "## Use Case", "## Evaluation", "### BLEU score:"]}, {"LoneStriker/BioMistral-7B-GPTQ": []}, {"skuma307/MedPaxTral-2x7b": []}, {"Marcuswas/bert-drug-review-to-condition": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"AshtonIsNotHere/GatorTron-OG-bc-ctr-nli": ["## Model description", "### Framework versions"]}, {"ltmai/Bio_ClinicalBERT_DDI_finetuned": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"DunnBC22/yolos-small-Blood_Cell_Object_Detection": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"bcwarner/audit-icu-llama-112_0M": []}, {"bcwarner/audit-icu-llama-219_8M": []}, {"LeroyDyer/Mixtral_AI_LCARS_WORLD_ARCHIVE-Q4_K_M-GGUF": ["## Use with llama.cpp"]}, {"sauriopqno/autotrain-enfermedadespt2-44370111920": ["## Validation Metrics"]}, {"Dr-BERT/DrBERT-7GB-Large": ["## 3.1 Install dependencies", "## 3.2 Download NACHOS Dataset text file", "## 3.3 Build your own tokenizer from scratch based on NACHOS", "## 3.4 Preprocessing and tokenization of the dataset", "## 3.5 Model training", "### 3.5.1 Pre-training from scratch", "### 3.5.2 continue pre-training"]}, {"chaoyi-wu/PMC_LLAMA_7B_10_epoch": []}, {"Ariel4/biobert-embeddings": []}, {"DunnBC22/yolos-small-Axial_MRIs": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"DATEXIS/clinical-assertion-negation-bert": ["## Model description", "#### How to use the model", "### Cite"]}, {"bcwarner/audit-icu-gpt2-131_6M": []}, {"bcwarner/audit-icu-gpt2-46_5M": []}, {"bcwarner/audit-icu-gpt2-89_0M": []}, {"aarnow/distilbert-base-uncased-1212-test": []}, {"EnDevSols/tinyllama-2.5T-Clinical": ["#### Eval"]}, {"Henrychur/MMedLM2-1_8B": ["## Introduction", "## News", "## Evaluation on MMedBench", "## Contact", "## Citation"]}, {"as-cle-bert/segformer-v1-breastcancer": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"PantagrueLLM/jargon-biomed-4096": ["## Evaluation", "## Using Jargon models with HuggingFace transformers", "## Citation", "### Model Sources [optional]"]}, {"AshtonIsNotHere/GatorTron-OG-breast-cancer": ["### GatorTron-OG-breast-cancer"]}, {"catlove007/Med-ChatGLM-6B": []}, {"DunnBC22/yolos-small-Abdomen_MRI": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"Mreeb/Medi-llama-2-7b-custom1000": ["## Model Details", "### Model Description"]}, {"LoneStriker/BioMistral-7B-3.0bpw-h6-exl2": []}, {"starmpcc/Asclepius-Llama2-13B-Pretraining-Only": ["## UPDATE", "### 2024.01.10", "## Model Details", "### Model Description", "### Model Sources", "## Uses", "### Out-of-Scope Use", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Training Hyperparameters", "#### Speeds, Sizes, Times", "## Citation"]}, {"hus960/base-7b-v0.2-Q4_K_M-GGUF": ["## Use with llama.cpp"]}, {"Svngoku/BioMistral-7B-Q4_K_M-GGUF": ["## Use with llama.cpp", "### CLI:", "### Server:"]}, {"psgrghvuo/roberta-base-qic": ["### Training hyperparameters", "### Framework versions"]}, {"qanastek/biomedical-specialities-classifier-french": []}, {"DunnBC22/bert-base-cased-finetuned-ner-NCBI_Disease": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"jayantdocplix/blokeAI-13b": ["## Repositories available", "## THE FILES IN MAIN BRANCH REQUIRES LATEST LLAMA.CPP (May 19th 2023 - commit 2d5db48)!", "## Provided files", "## How to run in `llama.cpp`", "## How to run in `text-generation-webui`", "## Discord", "## Thanks, and how to contribute.", "## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Model Usage", "## Limitations"]}, {"DunnBC22/yolos-small-Stomata_Cells": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"PetraAI/Nashmi": ["### Inference Speed", "### Perplexity", "## Installation", "### Quick Installation", "#### disable cuda extensions", "#### to support triton speedup", "### Install from source", "## Quick Tour", "### Quantization and Inference", "### Customize Model", "### Evaluation on Downstream Tasks", "## Learn More", "## Supported Models", "## Supported Evaluation Tasks", "## Running tests", "## Acknowledgement"]}, {"bcwarner/audit-icu-llama-58_1M": []}, {"surya47/medclip-roco": []}, {"ml4pubmed/xtremedistil-l12-h384-uncased_pub_section": ["## usage in python", "## metadata", "### training_parameters"]}, {"psybertpt/psyBERTpt": ["## NER Categories:", "## Acknowledgements", "## Citation", "## Questions?"]}, {"ZhangCNN/MindGLM": []}, {"bcwarner/audit-icu-rwkv-127_2M": []}, {"bcwarner/audit-icu-rwkv-65_7M": []}, {"TheBloke/medicine-LLM-13B-AWQ": ["## Description", "### About AWQ", "## Repositories available", "## Prompt template: Llama-2-Chat", "## Provided files, and AWQ parameters", "## How to easily download and use this model in [text-generation-webui](https://github.com/oobabooga/text-generation-webui)", "## Multi-user inference server: vLLM", "## Multi-user inference server: Hugging Face Text Generation Inference (TGI)", "## Inference from Python code using Transformers", "### Install the necessary packages", "### Transformers example code (requires Transformers 4.35.0 and later)", "## Compatibility", "## Discord", "## Thanks, and how to contribute", "### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"janduplessis886/clinical_text_classification": ["## Usage ", "## Topic overview", "## Training hyperparameters", "## Framework versions"]}, {"bcwarner/PubMedBERT-base-uncased-sts-combined": ["## Citation"]}, {"BMRetriever/BMRetriever-410M": ["## Usage", "## Citation"]}, {"LeroyDyer/Mixtral_AI_MasterMind-Q2_K-GGUF": ["## Use with llama.cpp"]}, {"anslin-raj/temp-llama-3-8b-16bit": []}, {"DaizeDong/GraphsGPT-1W-C": []}, {"opentargets/clinical_trial_stop_reasons": ["## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"rttl-ai/BIOptimus": ["## Model Details"]}, {"TigerResearch/medical-bot-peft-from-tigerbot-7b-sft": []}, {"IVN-RIN/MedPsyNIT": []}, {"ryefoxlime/PneumoniaDetection": ["## Model Details", "### Model Description", "### Model Sources", "## Uses", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Building the model", "#### Fitting the model", "#### Preprocessing", "#### Training Hyperparameters", "#### Define Defaults", "#### Metrics", "### Results", "#### Summary"]}, {"PantagrueLLM/jargon-NACHOS-4096": ["## Evaluation", "## Biomedical Benchmark", "## Using Jargon models with HuggingFace transformers", "## Citation", "### Model Sources [optional]"]}, {"longisland3/MMed-Llama-3-8B-gguf": ["## Introduction", "## News", "## Evaluation on MMedBench", "## Contact", "## Citation"]}, {"psgrghvuo/pubmedbert_bc5cdr": ["## Examples from BC5CDR (Test Set)", "### Training hyperparameters", "### Framework versions"]}, {"Pavarissy/mentalgpt-v0.0.1": []}, {"pszemraj/karnold-walmer-base-biopapers": ["## Intended Uses & Limitations", "## Training and Evaluation Data", "### Wordcloud ", "## Training procedure", "### Training hyperparameters", "### Training results"]}, {"DunnBC22/mbart-large-50-Biomedical_Dataset": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "### Histogram of English Input Word Counts", "### Histogram of Italian Input Word Counts", "## Training procedure", "### Training hyperparameters", "### Training results*", "### Framework versions"]}, {"xtie/OPT-PET-impression": []}, {"DATEXIS/CORe-clinical-outcome-biobert-v1": ["## Model description", "#### How to use CORe", "### Pre-Training Data", "### More Information", "### Cite"]}, {"ShayDuane/DeBERTa-V2-FT-EMR-data": []}, {"parthbelose/llama_2_7B_chat_fine_tuned_": []}, {"LeroyDyer/Mixtral_AI_Vision-Instruct_X": ["## LeroyDyer/Mixtral_AI_Vision-Instruct_X", "## Vision/multimodal capabilities:", "## Extended capabilities:", "## using transformers", "## Using pipeline", "## Mistral ChatTemplating"]}, {"jihadzakki/idefics2-8b-medvqa": []}, {"HaiderSultanArc/UnaniBERT": ["## Model Details", "### Model Description", "## Uses"]}, {"camila-ud/DrBERT-CASM2": ["## Model description", "## Limitations and bias", "## Install medkit", "## Using the model", "## Training procedure", "## How to evaluate using medkit"]}, {"abhinand/BioMedGPT-LM-7B-sharded-bf16": ["### Training Details", "### Model Developers", "### How to Use", "### Technical Report", "### GitHub", "### Limitations", "### Licenses"]}, {"Zamoranesis/delirium_roberta": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"phlobo/xmen-de-ce-medmentions": []}, {"jiey2/DISC-MedLLM": ["## Overview", "## Dataset", "## Deploy", "### Using through hugging face transformers", "## Training", "## Delcaration", "## Licenses", "## Citation"]}, {"HumaP/vit-base-patch16-224-in21k-lung_and_colon_histopathology": ["## Description  ", "## About Dataset", "## Training Results"]}, {"FremyCompany/BioLORD-2023-S": ["## Sibling models", "## Training strategy", "### Summary of the 3 phases", "### Contrastive phase: details", "### Self-distallation phase: details", "## Citation", "## Usage (Sentence-Transformers)", "## Usage (HuggingFace Transformers)", "## License"]}, {"starmpcc/Asclepius-Llama2-7B-Pretraining-Only": ["## UPDATE", "### 2024.01.10", "## Model Details", "### Model Description", "### Model Sources", "## Uses", "### Out-of-Scope Use", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Training Hyperparameters", "#### Speeds, Sizes, Times", "## Citation"]}, {"PantagrueLLM/jargon-general-biomed": ["## Evaluation", "## Biomedical Benchmark", "## Using Jargon models with HuggingFace transformers", "## Citation", "### Model Sources [optional]"]}, {"IDEA-CCNL/YuyuanQA-GPT2-3.5B": ["## \u7b80\u4ecb Brief Introduction", "## \u6a21\u578b\u5206\u7c7b Model Taxonomy", "## \u6a21\u578b\u4fe1\u606f Model Information", "### \u4e0b\u6e38\u4efb\u52a1 Performance", "## \u4f7f\u7528 Usage", "### \u52a0\u8f7d\u6a21\u578b Loading Models", "### \u4f7f\u7528\u793a\u4f8b Usage Examples", "### \u6f14\u793a Demo", "## \u5f15\u7528 Citation"]}, {"cogint/in-boxbart": []}, {"ludolara/vit-COVID-19-severity": ["## Diagnosing Medical Images for COVID-19 Severity (Regression task)", "## Model description"]}, {"chizhikchi/sci-five-radsum23": []}, {"youngp5/skin-conditions": []}, {"botch/Llama-2-7b-pubmed": ["## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"atwine/translation-en-lug-v2": []}, {"winninghealth/WiNGPT2-14B-Base": ["## WiNGPT2", "## \u4ecb\u7ecd", "## \u7279\u70b9", "## \u5982\u4f55\u4f7f\u7528", "### \u63a8\u7406", "## \u8f93\u51fa\u7ed3\u679c\uff1a\u4f60\u597d\uff01\u4eca\u5929\u6211\u80fd\u4e3a\u4f60\u505a\u4e9b\u4ec0\u4e48\uff1f<|endoftext|>", "### \u63d0\u793a", "### \u4f01\u4e1a\u670d\u52a1", "## \u8bad\u7ec3\u6570\u636e", "## \u6a21\u578b\u5361", "## \u8bc4\u6d4b", "## \u5c40\u9650\u6027\u4e0e\u514d\u8d23\u58f0\u660e", "## \u8bb8\u53ef\u8bc1", "## \u53c2\u8003\u8d44\u6599", "## \u8054\u7cfb\u6211\u4eec"]}, {"Ram07/emp1_dialog": []}, {"LeroyDyer/LCARS_WORLD_LIBRARY_ARCHIVES": []}, {"IndianaUniversityDatasetsModels/test-BB2": ["## Inputs and Outputs", "## Model Details", "### Model Description", "### Model Sources [optional]", "## Uses", "### Direct Use", "### Downstream Use [optional]", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "### How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Examination [optional]", "## Environmental Impact", "## Technical Specifications [optional]", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "#### Software", "## Citation [optional]", "## Glossary [optional]", "## More Information [optional]", "## Model Card Authors [optional]", "## Model Card Contact"]}, {"ronoys/PyRX": []}, {"abymmathew/RoBERTa-large-PM-M3-Voc-hf-finetuned-ner": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"Zamoranesis/clinical_bert": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters", "### Training results", "### Framework versions"]}, {"Tonic/GaiaMiniMed": ["## Model Details", "### Model Description", "### Model Sources ", "## Uses", "### Direct Use", "### Downstream Use", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "## How to Get Started with the Model", "## Training Details", "### Results", "### Training Data", "### Training Procedure ", "#### Preprocessing [optional]", "#### Training Hyperparameters", "#### Speeds, Sizes, Times [optional]", "## Environmental Impact", "## Technical Specifications", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "## Model Card Authors ", "## Model Card Contact"]}, {"HiTZ/Medical-mT5-xl": ["## How to Get Started with the Model", "## Training Data", "## Evaluation", "### Medical mT5 for Sequence Labelling", "### Single-task supervised F1 scores for Sequence Labelling", "### Multi-task supervised F1 scores for Sequence Labelling", "### Zero-shot F1 scores for Argument Mining. Models have been trained in English and evaluated in Spanish, French and Italian.", "## Ethical Statement", "## Citation"]}, {"cxllin/StableMed-3b": ["## Usage", "### Model Architecture"]}, {"NepaliAI/NFT-6.9k": ["## Model Details", "### Model Description", "### Intended Use", "### Training Data", "### Training Procedure", "## Use case: ", "## Evaluation", "### Metrics", "### Limitations", "## Ethical Considerations", "### Intended Use", "### Bias", "### Privacy", "## Future Directions", "## License"]}, {"hamxea/Llama-2-7b-chat-hf-activity-fine-tuned": ["## Model details", "## Intended use", "## Factors", "## Metrics", "## Evaluation datasets", "## Training dataset", "## Quantitative analysis", "## Ethical considerations"]}, {"SCIR-HI/ada-t5-base": ["### Pre-trained T5-base model on PseudoMD-1M datasets.", "### Pre-training details", "### Example Usage", "### [Citation](https://arxiv.org/abs/2309.05203)"]}, {"SCIR-HI/ada-t5-small": ["### Pre-trained T5-small model on PseudoMD-1M datasets.", "### Pre-training details", "### Example Usage", "### [Citation](https://arxiv.org/abs/2309.05203)"]}, {"TheBloke/medicine-LLM-AWQ": ["## Description", "### About AWQ", "## Repositories available", "## Prompt template: AdaptLLM", "### User Input:", "### Assistant Output:", "## Provided files, and AWQ parameters", "## How to easily download and use this model in [text-generation-webui](https://github.com/oobabooga/text-generation-webui)", "## Multi-user inference server: vLLM", "### Assistant Output:", "## Multi-user inference server: Hugging Face Text Generation Inference (TGI)", "### Assistant Output:", "## Inference from Python code using Transformers", "### Install the necessary packages", "### Transformers example code (requires Transformers 4.35.0 and later)", "### Assistant Output:", "## Compatibility", "## Discord", "## Thanks, and how to contribute", "### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"abazoge/DrBERT-4096": ["### Model pretraining", "### Model Usage", "### Citation"]}, {"Isaak-Carter/J.O.S.I.E.-x-Hercules-3.1-Mistral-7B-only-spetial-tokens": ["## This is my Token customized Locutusque/Hercules-3.1-Mistral-7B model", "## Training Data", "### New added Special Tokens", "### New BOS and EOS Tokens", "## Origional Instruction Prompt Format:", "## Model Architecture:"]}, {"arun-shankar/GPT-2-covid-news-articles": []}, {"IndianaUniversityDatasetsModels/test-BB3": ["## Inputs and Outputs"]}, {"andrewbrown/gpt2-mi-reflector": ["### A gpt2 based motivational interviewing reflector"]}, {"Harshit655/whisper-small-en": []}, {"AiLab-IMCS-UL/lvmed": []}, {"nmitchko/medguanaco-65b-GPTQ": ["## Table of Contents", "## Model Description", "### Architecture", "### Training Data", "## Limitations"]}, {"shanover/disease_classifier_base": []}, {"katielink/llava-med-7b-pathvqa-delta": ["## Model Description ", "### Model Uses ", "#### Intended Use ", "#### Primary Intended Use ", "#### Out-of-Scope Use ", "### Data ", "### Limitations ", "## Install", "## Serving", "## Evaluation", "### Medical Visual Chat (GPT-assisted Evaluation)", "### Medical VQA", "#### - Prepare Data", "#### - Fine-tuning", "#### - Evaluation", "## Acknowledgement", "## Related Projects"]}, {"katielink/llava-med-7b-vqarad-delta": ["## Model Description ", "### Model Uses ", "#### Intended Use ", "#### Primary Intended Use ", "#### Out-of-Scope Use ", "### Data ", "### Limitations ", "## Install", "## Serving", "## Evaluation", "### Medical Visual Chat (GPT-assisted Evaluation)", "### Medical VQA", "#### - Prepare Data", "#### - Fine-tuning", "#### - Evaluation", "## Acknowledgement", "## Related Projects"]}, {"ryefoxlime/PneumoniaDetect1": ["## Model description", "## Intended uses & limitations", "## Training and evaluation data", "## Training procedure", "### Training hyperparameters"]}, {"winninghealth/WiNGPT2-14B-Chat": ["## WiNGPT2", "## \u4ecb\u7ecd", "## \u7279\u70b9", "## \u5982\u4f55\u4f7f\u7528", "### \u63a8\u7406", "## \u8f93\u51fa\u7ed3\u679c\uff1a\u4f60\u597d\uff01\u4eca\u5929\u6211\u80fd\u4e3a\u4f60\u505a\u4e9b\u4ec0\u4e48\uff1f<|endoftext|>", "### \u63d0\u793a", "### \u4f01\u4e1a\u670d\u52a1", "## \u8bad\u7ec3\u6570\u636e", "## \u6a21\u578b\u5361", "## \u8bc4\u6d4b", "## \u5c40\u9650\u6027\u4e0e\u514d\u8d23\u58f0\u660e", "## \u8bb8\u53ef\u8bc1", "## \u53c2\u8003\u8d44\u6599", "## \u8054\u7cfb\u6211\u4eec"]}, {"WinterSchool/Midefics": []}, {"rameye/1": []}, {"GeorgiaTech/scibert-generative-pubmedqa": []}, {"ShadNygren/BioTechFineTuneTest": []}, {"sethuiyer/Medichat-V2-Llama3-8B": ["### Model Composition and Features:", "### Models Merged", "### Configuration", "## Usage", "## Quants"]}, {"Monor/TCMNER": ["## Training data"]}, {"Narrativa/NarbioBART": ["## \ud83e\udda0 NarbioBART \ud83c\udfe5 ", "## Training details", "## [Evaluation metrics](https://huggingface.co/mrm8488/bart-bio-base-es/tensorboard?params=scalars#frame) \ud83e\uddfe", "## Benchmarks \ud83d\udd28", "## How to use with `transformers`", "## Citation"]}, {"abhiman23897/longformer_pico_model": []}, {"xtie/ClinicalT5-PET-impression": ["## \ud83d\udcd1 Model Description", "## \ud83d\udcd1 Abstract", "## \ud83d\ude80 Usage", "### \ud83d\udcca Performance Metrics", "### \ud83d\udca1 Highlights", "### \ud83d\udda5\ufe0f Hardware", "## \ud83d\udcc1 Additional Resources"]}, {"Nitsuke/falcon-7b-instruct-ft": []}, {"SumayyaAli/llama-2-7b-chat-medical": []}, {"hamxea/Llama-2-7b-chat-hf-activity-fine-tuned-v2": ["## Model details", "## Intended use", "## Factors", "## Metrics", "## Evaluation datasets", "## Training dataset", "## Quantitative analysis", "## Ethical considerations"]}, {"alibidaran/llama-2-7b-virtual_doctor": []}, {"TheBloke/medicine-chat-AWQ": ["## Description", "### About AWQ", "## Repositories available", "## Prompt template: Llama-2-Chat", "## Provided files, and AWQ parameters", "## How to easily download and use this model in [text-generation-webui](https://github.com/oobabooga/text-generation-webui)", "## Multi-user inference server: vLLM", "## Multi-user inference server: Hugging Face Text Generation Inference (TGI)", "## Inference from Python code using Transformers", "### Install the necessary packages", "### Transformers example code (requires Transformers 4.35.0 and later)", "## Compatibility", "## Discord", "## Thanks, and how to contribute", "### \ud83e\udd17 We are currently working hard on developing models across different domains, scales and architectures! Please stay tuned! \ud83e\udd17", "## Domain-Specific LLaMA-1", "### LLaMA-1-7B", "### LLaMA-1-13B", "## Domain-Specific LLaMA-2-Chat", "## Domain-Specific Tasks", "## Citation"]}, {"muzammil-eds/tinyllama-2.5T-Clinical-v2": []}, {"sidushdid/ViT-base-patch16-BUSI-Mendeley-Adafactor": []}, {"sidushdid/ViT-base-patch16-BUSI-Mendeley-Lion": []}, {"pillIdentifierAI/pillIdentifier": []}, {"disi-unibo-nlp/MedGENIE-fid-flan-t5-base-medqa": ["## Model description", "## Performance", "### Training hyperparameters", "### Bias, Risk and Limitation", "## Citation"]}, {"solidrust/Hercules-3.1-Mistral-7B-AWQ": ["## Model Description", "## How to use", "### Install the necessary packages", "### Example Python code", "### About AWQ", "## Prompt template: ChatML"]}, {"woshiyuanshengaoshou/casrel": []}, {"terran-ai/WOS46985Bert": ["## Model Details", "### Model Description", "## Evaluation", "### Results", "#### Summary"]}, {"PetraAI/Zalmati": ["## Overview", "## Model Architecture", "## Intended Use  ", "## Limitations and Risks", "## How to Use"]}, {"bluesky333/medphi2": ["## Model Summary", "## How to Use", "## Intended Uses", "### QA Format:", "### Chat Format:", "### Code Format:", "## Sample Code", "## Limitations of Phi-2", "## Training", "### Model", "### Software", "### License", "## Trademarks"]}, {"semaj83/scibert_finetuned_ctmatch": ["## Model Details", "### Model Description", "### Model Sources ", "## Uses", "### Direct Use", "### Downstream Use ", "## Bias, Risks, and Limitations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "#### Preprocessing ", "#### Training Hyperparameters", "## Evaluation", "## Model Card Authors ", "## Model Card Contact"]}, {"Pankti99/llama-2-7b-HealthCareMagic": []}, {"xtie/T5v1.1-PET-impression": ["## \ud83d\udcd1 Model Description", "## \ud83d\udcd1 Abstract", "## \ud83d\ude80 Usage", "### \ud83d\udcca Performance Metrics", "### \ud83d\udca1 Highlights", "### \ud83d\udda5\ufe0f Hardware", "## \ud83d\udcc1 Additional Resources"]}, {"xtie/Flan-T5-PET-impression": ["## \ud83d\udcd1 Model Description", "## \ud83d\udcd1 Abstract", "## \ud83d\ude80 Usage", "### \ud83d\udcca Performance Metrics", "### \ud83d\udca1 Highlights", "### \ud83d\udda5\ufe0f Hardware", "## \ud83d\udcc1 Additional Resources"]}]