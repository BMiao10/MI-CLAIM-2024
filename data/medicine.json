[{"NewstaR/StableGalen-6b": ["### User: {prompt}", "### Response:", "## T3: 1 Hour"]}, {"Zakia/gpt2-drugscom_depression_reviews-hq-v1": ["## Model Details", "### Model Description", "## Uses", "### Direct Use", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing", "#### Training Hyperparameters", "## Evaluation", "#### Metrics", "### Results", "### Evaluation Results", "## Technical Specifications", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "## Citation", "## More Information", "## Model Card Authors", "## Model Card Contact"]}, {"Zakia/distilbert-drugscom_depression_reviews": ["## Model Details", "### Model Description", "## Uses", "### Direct Use", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing", "#### Training Hyperparameters", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Preprocessing", "#### Metrics", "### Results", "## Technical Specifications", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "## Citation", "## Glossary", "## More Information", "## Model Card Authors", "## Model Card Contact"]}, {"sacreemure/med_t5_summ_ru": ["## Model Details", "### Model Description"]}, {"Zakia/gpt2-drugscom_depression_reviews": ["## Model Details", "### Model Description", "## Uses", "### Direct Use", "### Out-of-Scope Use", "## Bias, Risks, and Limitations", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Preprocessing", "#### Training Hyperparameters", "## Evaluation", "#### Metrics", "### Results", "## Technical Specifications", "### Model Architecture and Objective", "### Compute Infrastructure", "#### Hardware", "## Citation", "## More Information", "## Model Card Authors", "## Model Card Contact"]}, {"ChenWeiLi/Taiwan-inquiry_7B_v2.1": ["### Model Description", "### Usage of the model", "### Model evaluation", "### DEMO"]}, {"ChenWeiLi/Taiwan-inquiry_7B_v2.0": ["### Model Description", "### Usage of the model", "### Demo"]}, {"Vidharshana/tamil-bert4MLM": []}, {"phi0112358/PMC_LLaMA-7B-ggml": ["### That's it!"]}]