[{"cambridgeltl/SapBERT-from-PubMedBERT-fulltext": ["### SapBERT-PubMedBERT", "### Expected input and output", "#### Extracting embeddings from SapBERT", "### Citation"]}, {"StanfordAIMI/RadBERT": ["## Citation"]}, {"StanfordAIMI/stanford-deidentifier-base": ["## Citation"]}, {"almanach/camembert-bio-base": ["## Absract", "## Training Details", "### Training Data", "### Training Procedure ", "## Evaluation", "### Fine-tuning", "### Scoring", "### Results ", "## Environmental Impact estimation", "## Citation information"]}, {"GanjinZero/UMLSBert_ENG": []}, {"xmcmic/Med-KEBERT": []}, {"PlanTL-GOB-ES/bsc-bio-ehr-es": ["## Table of contents", "## Model description", "## Intended uses and limitations", "## How to use", "## Limitations and bias", "## Training", "### Tokenization and model pretraining", "### Training corpora and preprocessing", "## Evaluation ", "## Additional information", "### Author", "### Contact information", "### Copyright", "### Licensing information", "### Funding", "### Citing information", "### Disclaimer"]}, {"Dr-BERT/DrBERT-7GB": ["## 3.1 Install dependencies", "## 3.2 Download NACHOS Dataset text file", "## 3.3 Build your own tokenizer from scratch based on NACHOS", "## 3.4 Preprocessing and tokenization of the dataset", "## 3.5 Model training", "### 3.5.1 Pre-training from scratch", "### 3.5.2 continue pre-training"]}, {"GanjinZero/biobart-v2-large": []}, {"GanjinZero/coder_eng": []}, {"StanfordAIMI/stanford-deidentifier-with-radiology-reports-and-i2b2": ["## Citation"]}, {"PlanTL-GOB-ES/bsc-bio-ehr-es-pharmaconer": ["## Table of contents", "## Model description", "## Intended uses and limitations", "## How to use", "## Limitations and bias", "## Training", "## Evaluation ", "## Additional information", "### Author", "### Contact information", "### Copyright", "### Licensing information", "### Funding", "## Citing information", "### Disclaimer"]}, {"GanjinZero/coder_all": []}, {"PlanTL-GOB-ES/roberta-base-biomedical-clinical-es": ["## Table of contents", "## Model description", "## Intended uses and limitations", "## How to use", "## Limitations and bias", "## Training", "## Evaluation ", "## Additional information", "### Author", "### Contact information", "### Copyright", "### Licensing information", "### Funding", "### Citation information", "### Disclaimer"]}, {"GanjinZero/biobart-large": []}, {"PlanTL-GOB-ES/bsc-bio-es": ["## Table of contents", "## Model description", "## Intended uses and limitations", "## How to use", "## Limitations and bias", "## Training", "### Tokenization and model pretraining", "### Training corpora and preprocessing", "## Evaluation ", "## Additional information", "### Author", "### Contact information", "### Copyright", "### Licensing information", "### Funding", "### Citation information", "### Disclaimer"]}, {"PlanTL-GOB-ES/roberta-base-biomedical-es": ["## Table of contents", "## Model description", "## Intended uses and limitations", "## How to use", "## Training", "### Tokenization and model pretraining", "### Training corpora and preprocessing", "## Evaluation", "## Additional information", "### Author", "### Contact information", "### Copyright", "### Licensing information", "### Funding", "## Citation information", "### Disclaimer"]}, {"StanfordAIMI/stanford-deidentifier-only-i2b2": ["## Citation"]}, {"abazoge/DrLongformer": ["### Model pretraining", "### Model Usage", "### Citation"]}, {"PlanTL-GOB-ES/longformer-base-4096-biomedical-clinical-es": ["## Table of contents", "## Model description", "## Intended uses and limitations", "## How to use", "## Limitations and bias", "## Training", "## Evaluation", "## Additional information", "### Author", "### Contact information", "### Copyright", "### Licensing information", "### Funding", "### Disclaimer"]}, {"whaleloops/keptlongformer": ["### Pre-training", "### Usage"]}, {"medkit/DrBERT-CASM2": ["## Model description", "## Limitations and bias", "## Install medkit", "## Using the model", "## Training procedure", "## How to evaluate using medkit"]}, {"whaleloops/KEPTlongformer-PMM3": ["### Pre-training", "### Usage"]}, {"nlpie/Llama2-MedTuned-7b": []}, {"nlpie/Llama2-MedTuned-13b": []}, {"BSC-LT/roberta-base-biomedical-es": ["## Tokenization and model pretraining", "## Training corpora and preprocessing", "## Evaluation and results", "## Intended uses & limitations", "## Cite", "## How to use"]}, {"BSC-NLP4BIA/biomedical-term-classifier-setfit": ["## Table of contents", "## Model description", "## Intended uses and limitations", "## How to use", "## Training", "## Evaluation", "## Additional information", "### Author", "### Licensing information", "### Citation information", "### Disclaimer"]}, {"Dr-BERT/DrBERT-4GB": ["## 3.1 Install dependencies", "## 3.2 Download NACHOS Dataset text file", "## 3.3 Build your own tokenizer from scratch based on NACHOS", "## 3.4 Preprocessing and tokenization of the dataset", "## 3.5 Model training", "### 3.5.1 Pre-training from scratch", "### 3.5.2 continue pre-training"]}, {"almanach/camembert-bio-gliner-v0.1": ["## Important", "## Installation", "## Usage", "## Links"]}, {"Dr-BERT/DrBERT-4GB-CP-PubMedBERT": ["## 3.1 Install dependencies", "## 3.2 Download NACHOS Dataset text file", "## 3.3 Build your own tokenizer from scratch based on NACHOS", "## 3.4 Preprocessing and tokenization of the dataset", "## 3.5 Model training", "### 3.5.1 Pre-training from scratch", "### 3.5.2 continue pre-training"]}, {"GanjinZero/coder_eng_pp": []}, {"kiddothe2b/biomedical-longformer-base": []}, {"StanfordAIMI/stanford-deidentifier-only-radiology-reports": ["## Citation"]}, {"GanjinZero/biobart-v2-base": []}, {"GanjinZero/biobart-base": []}, {"PlanTL-GOB-ES/bsc-bio-ehr-es-cantemist": ["## Table of contents", "## Model description", "## Intended uses and limitations", "## How to use", "## Limitations and bias", "## Training", "## Evaluation ", "## Additional information", "### Author", "### Contact information", "### Copyright", "### Licensing information", "### Funding", "### Citing information", "### Disclaimer"]}, {"ml4pubmed/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext_pub_section": ["## usage in python", "## metadata", "### training_metrics"]}, {"cimm-kzn/endr-bert": ["## EnDR-BERT"]}, {"cimm-kzn/rudr-bert": ["## RuDR-BERT", "## Citing & Authors"]}, {"andorei/BERGAMOT-multilingual-GAT": []}, {"StanfordAIMI/covid-radbert": ["## Citation"]}, {"StanfordAIMI/stanford-deidentifier-only-radiology-reports-augmented": ["## Citation"]}, {"pszemraj/long-t5-tglobal-base-16384-booksci-summary-v1": ["## Model Details", "## Usage ", "## Intended uses & limitations", "## Training procedure", "### Results ", "### Training hyperparameters", "### Training results"]}, {"BSC-LT/roberta-base-biomedical-clinical-es": ["## Tokenization and model pretraining", "## Training corpora and preprocessing", "## Evaluation and results", "## Intended uses & limitations", "## Cite", "## How to use"]}, {"hamzamalik11/Biobart_radiology_summarization": ["## Model Details", "### Model Description", "### Model Sources ", "## Uses", "### Direct Use", "### Out-of-Scope Use", "### Recommendations", "## How to Get Started with the Model", "## Training Details", "### Training Data", "### Training Procedure ", "#### Training Hyperparameters", "## Evaluation", "### Testing Data, Factors & Metrics", "#### Testing Data", "#### Factors", "#### Metrics", "### Results", "#### Summary", "## Model Card Authors ", "## Model Card Contact"]}, {"ml4pubmed/scibert-scivocab-uncased_pub_section": ["## usage in python", "## metadata", "### training_metrics", "### training_parameters"]}, {"IIC/xlm-roberta-large-distemist": ["## Parameters used", "## BibTeX entry and citation info"]}, {"balzanilo/UMLSBert_ENG": []}, {"cimm-kzn/enrudr-bert": ["## EnRuDR-BERT", "## Citing & Authors"]}, {"Dr-BERT/DrBERT-7GB-Large": ["## 3.1 Install dependencies", "## 3.2 Download NACHOS Dataset text file", "## 3.3 Build your own tokenizer from scratch based on NACHOS", "## 3.4 Preprocessing and tokenization of the dataset", "## 3.5 Model training", "### 3.5.1 Pre-training from scratch", "### 3.5.2 continue pre-training"]}]